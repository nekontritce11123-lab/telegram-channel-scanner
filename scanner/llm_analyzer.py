"""
LLM –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞–Ω–∞–ª–æ–≤ v47.0 (Grandmaster Edition)

–¢—Ä–∏ –º–æ–¥—É–ª—è:
1. AdAnalyzer ‚Äî % —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM)
2. CommentAnalyzer ‚Äî Bot Detection + Trust Score
3. BrandSafetyAnalyzer ‚Äî –¥–µ—Ç–µ–∫—Ü–∏—è —Ç–æ–∫—Å–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (v47.0)

v47.0 –∏–∑–º–µ–Ω–µ–Ω–∏—è (Brand Safety Grandmaster Edition):
- Chain-of-Thought (reasoning field) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
- Semantic severity ‚Äî "Dominant theme" –≤–º–µ—Å—Ç–æ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ (8B –º–æ–¥–µ–ª–∏ –ø–ª–æ—Ö–æ —Å—á–∏—Ç–∞—é—Ç)
- Context Traps ‚Äî —è–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã SAFE —Å–ª—É—á–∞–µ–≤ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è false positives
- De-obfuscation –∫–∞–∫ Core Principle (k@zin0 ‚Üí casino)
- Python —Å—á–∏—Ç–∞–µ—Ç —Ç–æ—á–Ω—ã–π toxic_ratio, –Ω–µ LLM

v46.0 –∏–∑–º–µ–Ω–µ–Ω–∏—è:
- –î–æ–±–∞–≤–ª–µ–Ω LLM Brand Safety –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (GAMBLING, ADULT, SCAM)
- LLM –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç, –æ–±—Ñ—É—Å–∫–∞—Ü–∏—é, —ç–≤—Ñ–µ–º–∏–∑–º—ã
- –ó–∞–º–µ–Ω—è–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —Ñ–∏–ª—å—Ç—Ä –¥–ª—è —É–º–Ω–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏

–ú–µ—Ç—Ä–∏–∫–∏:
- ad_percentage: % —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ ‚Üí ad_mult (0.4-1.0)
- bot_percentage: % –±–æ—Ç–æ–≤ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö ‚Üí bot_mult (0.3-1.0)
- trust_score: –¥–æ–≤–µ—Ä–∏–µ –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –∫ –∫–æ–Ω—Ç–µ–Ω—Ç—É
- brand_safety: —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ‚Üí safety_mult (0.0-1.0)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Ollama + Qwen3-8B
"""

import json
import asyncio
import re
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any
from dataclasses import dataclass

import requests

# v2.1: –û–±—â–∏–µ —É—Ç–∏–ª–∏—Ç—ã (clean_text)
from scanner.utils import clean_text

# v23.0: unified cache from cache.py
from scanner.cache import get_llm_cache

# v43.0: –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
# v51.0: MAX_POSTS_FOR_AI, MAX_CHARS_PER_POST –¥–ª—è —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏
from scanner.config import (
    OLLAMA_URL,
    OLLAMA_MODEL,
    OLLAMA_TIMEOUT,
    MAX_RETRIES,
    RETRY_DELAY,
    MAX_POSTS_FOR_AI,
    MAX_CHARS_PER_POST,
)

# === V2.0: JSON REPAIR ===
try:
    from json_repair import repair_json
    HAS_JSON_REPAIR = True
except ImportError:
    HAS_JSON_REPAIR = False
    print("WARNING: json_repair not installed. pip install json-repair")


# === V2.0: JSON PARSING ===

def safe_parse_json(response: str, default_values: dict = None) -> tuple:
    """
    V2.0: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç LLM —Å multi-level fallback.

    Levels:
    1. Direct json.loads (find JSON object in text)
    2. json_repair library (fix trailing commas, etc.)
    3. Regex extraction of known fields

    Returns:
        (data, warnings): parsed dict and list of warnings
    """
    warnings = []

    if not response or not response.strip():
        warnings.append("Empty response from LLM")
        return default_values or {}, warnings

    # Level 1: Find and parse JSON object
    try:
        # Find balanced braces
        start_idx = response.find('{')
        if start_idx != -1:
            depth = 0
            end_idx = start_idx
            for i, char in enumerate(response[start_idx:], start_idx):
                if char == '{':
                    depth += 1
                elif char == '}':
                    depth -= 1
                    if depth == 0:
                        end_idx = i + 1
                        break

            json_candidate = response[start_idx:end_idx]
            data = json.loads(json_candidate)
            warnings.append("L1: Direct JSON parse succeeded")
            return _fill_defaults(data, default_values), warnings
    except json.JSONDecodeError as e:
        warnings.append(f"L1: JSON decode error - {e.msg}")
    except (IndexError, KeyError) as e:
        # IndexError: –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ response
        # KeyError: –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ dict –∫–ª—é—á–∞–º
        warnings.append(f"L1: Parse error - {e}")

    # Level 2: json_repair library
    if HAS_JSON_REPAIR:
        try:
            repaired = repair_json(response)
            if repaired:
                data = json.loads(repaired)
                warnings.append("L2: json_repair succeeded")
                return _fill_defaults(data, default_values), warnings
        except (json.JSONDecodeError, ValueError, TypeError) as e:
            # json_repair –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON –∏–ª–∏ —Å–ª–æ–º–∞—Ç—å—Å—è
            warnings.append(f"L2: json_repair failed - {e}")
    else:
        warnings.append("L2: json_repair not installed, skipping")

    # Level 3: Regex extraction
    try:
        data = _regex_extract_fields(response)
        if data:
            warnings.append(f"L3: Regex extracted {len(data)} fields")
            return _fill_defaults(data, default_values), warnings
    except (re.error, TypeError, AttributeError) as e:
        # re.error: –æ—à–∏–±–∫–∞ —Ä–µ–≥—É–ª—è—Ä–∫–∏, TypeError/AttributeError: –Ω–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö
        warnings.append(f"L3: Regex extraction failed - {e}")

    warnings.append("FAILED: All parsing levels exhausted")
    return default_values or {}, warnings


def _fill_defaults(data: dict, default_values: dict) -> dict:
    """Fill missing fields with defaults."""
    if not default_values:
        return data

    for key, default in default_values.items():
        if key not in data or data[key] is None:
            data[key] = default

    return data


def _regex_extract_fields(response: str) -> dict:
    """Level 3: Extract fields via regex patterns."""
    patterns = {
        "toxicity": r'"?toxicity"?\s*[:\s]+(\d+)',
        "violence": r'"?violence"?\s*[:\s]+(\d+)',
        "military_conflict": r'"?military_conflict"?\s*[:\s]+(\d+)',
        "political_quantity": r'"?political_quantity"?\s*[:\s]+(\d+)',
        "political_risk": r'"?political_risk"?\s*[:\s]+(\d+)',
        "misinformation": r'"?misinformation"?\s*[:\s]+(\d+)',
        "ad_percentage": r'"?ad_percentage"?\s*[:\s]+(\d+)',
    }

    data = {}
    for field, pattern in patterns.items():
        match = re.search(pattern, response, re.IGNORECASE)
        if match:
            try:
                data[field] = int(match.group(1))
            except ValueError:
                pass

    # Extract red_flags array
    flags_match = re.search(r'"?red_flags"?\s*:\s*\[(.*?)\]', response, re.DOTALL)
    if flags_match:
        content = flags_match.group(1).strip()
        data["red_flags"] = re.findall(r'"([^"]+)"', content) if content else []

    return data if data else None


# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===
# v43.0: OLLAMA_URL, OLLAMA_MODEL, OLLAMA_TIMEOUT, MAX_RETRIES, RETRY_DELAY
# –∏–º–ø–æ—Ä—Ç–∏—Ä—É—é—Ç—Å—è –∏–∑ scanner.config
# v23.0: –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ unified cache.py (get_llm_cache)

# DEBUG
DEBUG_LLM_ANALYZER = False  # v41.0: –æ—Ç–∫–ª—é—á–µ–Ω –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞

# –õ–∏–º–∏—Ç—ã (v51.0: —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã —á–µ—Ä–µ–∑ config.py)
MAX_POSTS_FOR_ANALYSIS = MAX_POSTS_FOR_AI  # –∏–∑ config.py (50)
MAX_COMMENTS_FOR_ANALYSIS = 999  # v40.2: –±–µ–∑ –ª–∏–º–∏—Ç–∞ (—Å–∫–æ–ª—å–∫–æ API –¥–∞—ë—Ç)
# MAX_CHARS_PER_POST —Ç–µ–ø–µ—Ä—å –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç—Å—è –∏–∑ config.py (800)


# === –†–ï–ó–£–õ–¨–¢–ê–¢–´ ===

@dataclass
class PostAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–æ–≤ V2.0"""
    brand_safety: int          # 0-100 (100 = –±–µ–∑–æ–ø–∞—Å–Ω–æ), –í–´–ß–ò–°–õ–Ø–ï–¢–°–Ø –í PYTHON
    toxicity: int              # 0-100 (hate speech, –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è)
    violence: int              # 0-100 (–ø—Ä–∏–∑—ã–≤—ã –∫ –Ω–∞—Å–∏–ª–∏—é, –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç)
    military_conflict: int     # 0-100 (V2.0: –≤–æ–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç violence)
    political_quantity: int    # 0-100 (% –ø–æ—Å—Ç–æ–≤ —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π)
    political_risk: int        # 0-100 (–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
    misinformation: int        # 0-100
    ad_percentage: int         # 0-100%
    red_flags: list
    raw_response: str = ""
    _brand_details: dict = None  # V2.0: calculation breakdown for debugging

    # Backwards compatibility: property –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞
    @property
    def political(self) -> int:
        """–î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç political_risk"""
        return self.political_risk


@dataclass
class CommentAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ v41.0 (–±–µ–∑ authenticity)"""
    bot_percentage: int        # 0-100% (–≥–ª–∞–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
    bot_signals: list
    trust_score: int           # 0-100
    trust_signals: list
    raw_response: str = ""


@dataclass
class LLMAnalysisResult:
    """–ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç LLM –∞–Ω–∞–ª–∏–∑–∞ v46.0"""
    posts: Optional[PostAnalysisResult]
    comments: Optional[CommentAnalysisResult]

    # v46.0: Brand Safety —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    safety: Optional[dict] = None    # {is_toxic, toxic_category, severity, ...}

    # –†–∞—Å—á—ë—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    llm_bonus: float = 0.0           # +0-15 points
    llm_trust_factor: float = 1.0    # √ó0.15-1.0

    # v37.0: –¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
    tier: str = "PREMIUM"            # PREMIUM/STANDARD/LIMITED/RESTRICTED/EXCLUDED
    tier_cap: int = 100              # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –¥–ª—è —Ç–∏—Ä–∞
    exclusion_reason: Optional[str] = None  # –ü—Ä–∏—á–∏–Ω–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–µ—Å–ª–∏ EXCLUDED)

    # v46.0: –î–µ—Ç–∞–ª–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (ad_mult + bot_mult + safety_mult)
    _ad_mult: float = 1.0            # v41.0: –º–Ω–æ–∂–∏—Ç–µ–ª—å –æ—Ç ad_percentage
    _safety_mult: float = 1.0        # v46.0: –º–Ω–æ–∂–∏—Ç–µ–ª—å –æ—Ç Brand Safety
    _comment_mult: float = 1.0       # bot_mult
    _political_mult: float = 1.0

    def calculate_impact_v2(self):
        """
        V3.0 (v41.0): Unified LLM Trust Factor.

        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç ad_mult (–∏–∑ ad_percentage) –∏ bot_mult (–∏–∑ bot_percentage).
        –û–±–∞ —Ñ–∞–∫—Ç–æ—Ä–∞ –ø–µ—Ä–µ–º–Ω–æ–∂–∞—é—Ç—Å—è –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ llm_trust_factor.
        """
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        self.tier = "STANDARD"
        self.tier_cap = 85
        self.exclusion_reason = None
        self.llm_bonus = 5.0  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–æ–Ω—É—Å

        # --- Ad Multiplier (v45.0) ---
        # –£–∂–µ—Å—Ç–æ—á—ë–Ω–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è –∑–∞—â–∏—Ç—ã –±—é–¥–∂–µ—Ç–∞ —Ä–µ–∫–ª–∞–º–æ–¥–∞—Ç–µ–ª—è
        # –ü—Ä–∏ 40% —Ä–µ–∫–ª–∞–º—ã –∞—É–¥–∏—Ç–æ—Ä–∏—è —É–∂–µ "–≤—ã–∂–∂–µ–Ω–∞"
        ad_mult = 1.0
        if self.posts and self.posts.ad_percentage is not None:
            ad_pct = self.posts.ad_percentage
            if ad_pct <= 15:
                ad_mult = 1.0      # –ü—Ä–µ–º–∏—É–º –∫–∞–Ω–∞–ª
            elif ad_pct <= 20:
                ad_mult = 0.95     # –ù–æ—Ä–º–∞
            elif ad_pct <= 25:
                ad_mult = 0.85     # –ù–∞—á–∞–ª–æ —à—Ç—Ä–∞—Ñ–∞
            elif ad_pct <= 30:
                ad_mult = 0.70     # –ú–Ω–æ–≥–æ —Ä–µ–∫–ª–∞–º—ã
            elif ad_pct <= 40:
                ad_mult = 0.50     # –í—ã–∂–∂–µ–Ω–Ω–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
            elif ad_pct <= 50:
                ad_mult = 0.35     # –û—á–µ–Ω—å –º–Ω–æ–≥–æ
            else:
                ad_mult = 0.20     # –°–ø–∞–º
        self._ad_mult = ad_mult

        # --- Bot Multiplier (v40.3) ---
        bot_mult = 1.0
        if self.comments and self.comments.bot_percentage is not None:
            bot_pct = self.comments.bot_percentage
            if bot_pct <= 15:
                bot_mult = 1.0
            else:
                # –õ–∏–Ω–µ–π–Ω—ã–π —à—Ç—Ä–∞—Ñ –æ—Ç 15% –¥–æ 85%
                penalty = (bot_pct - 15) / 100.0
                bot_mult = max(0.3, 1.0 - penalty)
        self._comment_mult = bot_mult

        # --- Brand Safety Multiplier (v47.0) ---
        # SAFE/LOW = 1.0, MEDIUM = 0.7, HIGH = 0.3, CRITICAL = 0.0 (EXCLUDED)
        safety_mult = 1.0
        if self.safety and self.safety.get('is_toxic'):
            severity = self.safety.get('severity', 'SAFE')

            # –ú–Ω–æ–∂–∏—Ç–µ–ª–∏ –ø–æ severity_label (v47.0)
            if severity == 'CRITICAL':
                safety_mult = 0.0  # EXCLUDED
                self.tier = "EXCLUDED"
                self.tier_cap = 0
                self.exclusion_reason = f"TOXIC_{self.safety.get('toxic_category', 'CONTENT')}"
            elif severity == 'HIGH':
                safety_mult = 0.3
                self.tier = "RESTRICTED"
                self.tier_cap = 30
            elif severity == 'MEDIUM':
                safety_mult = 0.7
            # LOW/SAFE –Ω–µ —à—Ç—Ä–∞—Ñ—É–µ–º
        self._safety_mult = safety_mult

        # --- Combined LLM Trust Factor (v47.0) ---
        # –ü–µ—Ä–µ–º–Ω–æ–∂–∞–µ–º –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã —Å floor 0.15
        self.llm_trust_factor = max(0.15, ad_mult * bot_mult * safety_mult)

        # v47.0: EXCLUDED override (safety_mult = 0)
        if safety_mult == 0.0:
            self.llm_trust_factor = 0.0

        self._political_mult = 1.0

        if DEBUG_LLM_ANALYZER:
            print(f"üìä V46.0: ad={ad_mult:.2f}, bot={bot_mult:.2f}, safety={safety_mult:.2f} ‚Üí llm_trust={self.llm_trust_factor:.2f}")


# === –ö–≠–®–ò–†–û–í–ê–ù–ò–ï ===
# v23.0: unified cache from cache.py
# –°—Ç–∞—Ä—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ _load_cache/_save_cache —É–¥–∞–ª–µ–Ω—ã
# –ò—Å–ø–æ–ª—å–∑—É–µ–º get_llm_cache() –≤ –∫–ª–∞—Å—Å–µ LLMAnalyzer

# === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===

def _detect_footer(posts_texts: list, min_occurrences: int = 5) -> Optional[str]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ñ—É—Ç–µ—Ä –≤ –ø–æ—Å—Ç–∞—Ö.
    –§—É—Ç–µ—Ä = —Ç–µ–∫—Å—Ç –≤ –∫–æ–Ω—Ü–µ –ø–æ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –≤ min_occurrences –ø–æ—Å—Ç–∞—Ö.
    """
    if len(posts_texts) < min_occurrences:
        return None

    # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞
    endings = []
    for text in posts_texts:
        if len(text) > 50:
            endings.append(text[-200:])

    if len(endings) < min_occurrences:
        return None

    # –ò—â–µ–º –æ–±—â–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ –∫–æ–Ω—Ü–∞—Ö
    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: –∏—â–µ–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —á–∞—Å—Ç–æ
    from collections import Counter

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ –∏—â–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –±–ª–æ–∫–∏
    line_counts = Counter()
    for ending in endings:
        lines = ending.split('\n')
        for line in lines:
            line = line.strip()
            if len(line) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                line_counts[line] += 1

    # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ >50% –ø–æ—Å—Ç–æ–≤
    threshold = len(posts_texts) * 0.4
    footer_lines = [line for line, count in line_counts.items() if count >= threshold]

    if footer_lines:
        return footer_lines[0]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—É—é —á–∞—Å—Ç—É—é
    return None


def _remove_footer(text: str, footer: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç —Ñ—É—Ç–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not footer:
        return text

    # –ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É —Å —Ñ—É—Ç–µ—Ä–æ–º –∏ –≤—Å—ë –ø–æ—Å–ª–µ –Ω–µ—ë
    idx = text.find(footer)
    if idx > 0:
        return text[:idx].strip()
    return text


def _prepare_posts_text(messages: list) -> str:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ v35.1"""
    # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
    raw_texts = []
    for msg in messages[:MAX_POSTS_FOR_ANALYSIS]:
        text = ""
        if hasattr(msg, 'message') and msg.message:
            text = msg.message
        elif hasattr(msg, 'text') and msg.text:
            text = msg.text
        if text and len(text) > 30:
            raw_texts.append(clean_text(text))

    # –î–µ—Ç–µ–∫—Ç–∏–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ñ—É—Ç–µ—Ä
    footer = _detect_footer(raw_texts)
    if footer and DEBUG_LLM_ANALYZER:
        print(f"DETECTED FOOTER: '{footer[:50]}...'")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã –±–µ–∑ —Ñ—É—Ç–µ—Ä–∞
    posts = []
    for i, text in enumerate(raw_texts):
        clean = _remove_footer(text, footer)[:MAX_CHARS_PER_POST]
        if clean and len(clean) > 30:
            posts.append(f"[Post {i+1}]: {clean}")

    return "\n\n".join(posts)


def _prepare_comments_text(comments: list) -> str:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    texts = []
    for i, comment in enumerate(comments[:MAX_COMMENTS_FOR_ANALYSIS]):
        text = ""
        if hasattr(comment, 'message') and comment.message:
            text = comment.message
        elif hasattr(comment, 'text') and comment.text:
            text = comment.text
        elif isinstance(comment, str):
            text = comment

        if text:
            clean = clean_text(text)[:200]
            if clean and len(clean) > 5:
                texts.append(f"[{i+1}]: {clean}")

    return "\n".join(texts)


# === OLLAMA API ===

def _call_ollama(system_prompt: str, user_message: str, retry_count: int = 0) -> Optional[str]:
    """
    –ó–∞–ø—Ä–æ—Å –∫ Ollama —Å retry –ª–æ–≥–∏–∫–æ–π.
    v43.0: –î–æ–±–∞–≤–ª–µ–Ω retry –ø—Ä–∏ —Ç–∞–π–º–∞—É—Ç–∞—Ö (–∏–∑ classifier.py).
    """
    payload = {
        "model": OLLAMA_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ],
        "stream": False,
        "think": False,
        "keep_alive": -1,  # v38.0: –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—ã–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏
        "options": {
            "temperature": 0.3,
            "num_predict": 500
        }
    }

    try:
        response = requests.post(OLLAMA_URL, json=payload, timeout=OLLAMA_TIMEOUT)
        if response.status_code != 200:
            print(f"OLLAMA: HTTP {response.status_code}")
            return None

        data = response.json()
        content = data.get("message", {}).get("content", "").strip()
        return content

    except requests.exceptions.ConnectionError:
        print("OLLAMA: –ù–µ –∑–∞–ø—É—â–µ–Ω! –ó–∞–ø—É—Å—Ç–∏: ollama serve")
        return None
    except requests.exceptions.Timeout:
        if retry_count < MAX_RETRIES:
            wait = RETRY_DELAY * (retry_count + 1)
            print(f"OLLAMA: –¢–∞–π–º–∞—É—Ç, retry {retry_count + 1}/{MAX_RETRIES} —á–µ—Ä–µ–∑ {wait}—Å...")
            time.sleep(wait)
            return _call_ollama(system_prompt, user_message, retry_count + 1)
        print(f"OLLAMA: –¢–∞–π–º–∞—É—Ç –ø–æ—Å–ª–µ {MAX_RETRIES} –ø–æ–ø—ã—Ç–æ–∫!")
        return None
    except (KeyError, TypeError, ValueError) as e:
        # KeyError: –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ JSON –æ—Ç–≤–µ—Ç–∞
        # TypeError/ValueError: –æ—à–∏–±–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        print(f"OLLAMA: –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ - {e}")
        return None


# === CHANNEL TYPE DETECTION V40.0 ===

CHANNEL_TYPE_KEYWORDS = {
    "TECH": ["python", "javascript", "–∫–æ–¥", "–ø—Ä–æ–≥—Ä–∞–º–º", "dev", "api", "github", "npm", "docker", "react", "vue", "backend", "frontend", "llm", "ml", "ai", "–Ω–µ–π—Ä–æ—Å–µ—Ç", "–º–æ–¥–µ–ª", "gpt", "ollama", "gguf"],
    "CRYPTO": ["btc", "eth", "–∫—Ä–∏–ø—Ç", "—Ç–æ–∫–µ–Ω", "defi", "nft", "–±–ª–æ–∫—á–µ–π–Ω", "–±–∏—Ç–∫–æ–∏–Ω", "—ç—Ñ–∏—Ä", "coin", "swap", "airdrop", "wallet"],
    "NEWS": ["–Ω–æ–≤–æ—Å—Ç–∏", "news", "—Å—Ä–æ—á–Ω–æ", "breaking", "–ø–æ–ª–∏—Ç–∏–∫", "—ç–∫–æ–Ω–æ–º–∏–∫", "–∏–Ω—Ñ–ª—è—Ü", "–∫—É—Ä—Å –≤–∞–ª—é—Ç"],
    "ENTERTAINMENT": ["–º–µ–º", "—é–º–æ—Ä", "–ø—Ä–∏–∫–æ–ª", "—Å–º–µ—à–Ω", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "–∏–≥—Ä", "anime", "–∞–Ω–∏–º–µ", "–º—É–∑—ã–∫"],
    "BUSINESS": ["–±–∏–∑–Ω–µ—Å", "—Å—Ç–∞—Ä—Ç–∞–ø", "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "–ø—Ä–æ–¥–∞–∂", "—Ñ–∏–Ω–∞–Ω—Å", "–∏–Ω–≤–µ—Å—Ç–∏—Ü", "–∞–∫—Ü–∏"],
}


def infer_channel_type(messages: list = None, category: str = None) -> str:
    """
    V40.0: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–∞–Ω–∞–ª–∞ –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤.

    Args:
        messages: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)
        category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∏–∑ classifier (TECH, AI_ML, CRYPTO, etc.)

    Returns:
        str: TECH, CRYPTO, NEWS, ENTERTAINMENT, BUSINESS, –∏–ª–∏ GENERAL
    """
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: Explicit category from classifier
    if category:
        cat_upper = category.upper()
        if cat_upper in ["TECH", "AI_ML", "EDUCATION"]:
            return "TECH"
        if cat_upper == "CRYPTO":
            return "CRYPTO"
        if cat_upper == "NEWS":
            return "NEWS"
        if cat_upper in ["ENTERTAINMENT", "LIFESTYLE"]:
            return "ENTERTAINMENT"
        if cat_upper in ["BUSINESS", "FINANCE", "REAL_ESTATE"]:
            return "BUSINESS"

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    if not messages:
        return "GENERAL"

    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–µ—Ä–≤—ã—Ö 20 –ø–æ—Å—Ç–æ–≤
    text_blob = ""
    for msg in messages[:20]:
        if hasattr(msg, 'message') and msg.message:
            text_blob += msg.message.lower() + " "
        elif hasattr(msg, 'text') and msg.text:
            text_blob += msg.text.lower() + " "

    if not text_blob:
        return "GENERAL"

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ —Ç–∏–ø–∞–º
    scores = {}
    for ctype, keywords in CHANNEL_TYPE_KEYWORDS.items():
        scores[ctype] = sum(1 for kw in keywords if kw in text_blob)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∏–ø —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º score (–µ—Å–ª–∏ >= 3 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π)
    if scores:
        best_type = max(scores, key=scores.get)
        if scores[best_type] >= 3:
            return best_type

    return "GENERAL"


# === BRAND SAFETY ANALYZER V47.1 (Crypto Trading Context Trap) ===

BRAND_SAFETY_SYSTEM = """You are a Brand Safety Auditor (V47.1).
Your mission is to protect advertisers from placing ads in TOXIC channels (Gambling, Adult, Scam).
You analyze semantic meaning, ignoring obfuscation (leetspeak) and respecting context.

## üö´ TOXIC CATEGORIES (DETECTION RULES)

### 1. GAMBLING üé∞
- **Targets:** Online casinos, slots, roulette, sports betting, bookmakers.
- **Keywords:** k@zino, sl0ts, 1win, 1xbet, stake, "raising money", "guaranteed profit schemes", "spin".
- **CONTEXT TRAP (SAFE - DO NOT FLAG):**
    - **Crypto Trading/Scalping:** Futures, Long/Short positions, Leverage (x100), Liquidation ("–ª–∏–∫–≤–∏–¥–Ω—É–ª—Å—è", "rekt"), "Catching a move/knife".
    - **Exchanges (Safe):** Binance, Bybit, MEXC ("–º–µ–∫—Å"), OKX, BingX. (These are NOT casinos).
    - **Trading Slang:** "Algo trading" ("–∞–ª–≥–æ—Å—ã"), "Screener", "Setup", "Spread 10$" ("—Ä–∞—Å–∫–∏–Ω—É—Ç—å", meaning diversify).
    - **Stock Market:** IPO, volatility, dividends.
- **VERDICT:** Only flag if the post promotes CASINOS or BOOKMAKERS. Trading on exchanges is SAFE.

### 2. ADULT üîû
- **Targets:** Pornography, Hentai, Escort, OnlyFans (NSFW).
- **Keywords:** p0rn, s.e.x, nudes, "hot girls", —ç—Å–∫–æ—Ä—Ç, –∏–Ω—Ç–∏–º.
- **CONTEXT TRAP (SAFE):** Dating advice, medical health discussions, relationship psychology, art/anime (non-explicit).
- **CRITICAL:** Any content involving minors = IMMEDIATE CRITICAL.
- **VERDICT:** Only flag if the intent is sexual arousal or soliciting services.

### 3. SCAM ‚ö†Ô∏è
- **Targets:** Darknet markets, drugs, carding, cash-out schemes, fake documents.
- **Keywords:** –¥–∞—Ä–∫–Ω–µ—Ç, –∫–ª–∞–¥–º–µ–Ω, –æ–±–Ω–∞–ª, –¥—Ä–æ–ø—ã, CVV, fullz.
- **CONTEXT TRAP (SAFE):** True crime stories, cybersecurity education, crypto news, high-risk trading diaries (losing money on trading is NOT a scam).
- **VERDICT:** Only flag if the channel SELLS or PROMOTES illegal goods/services.

## üõ°Ô∏è CORE PRINCIPLES

1. **DE-OBFUSCATION:** Read "k@zin0" as "casino", "p0rn" as "porn".
2. **CONTEXT IS KING:** A trader losing money on "MEXC" is SAFE (High Risk Finance). A player winning on "1win" is TOXIC (Gambling).
3. **PATTERN OVER SINGLE POST:** One toxic post ‚â† toxic channel. Look for recurring themes.
4. **CONSERVATIVE:** If you are not 100% sure it's toxic, it is CLEAN.

## OUTPUT INSTRUCTION

1. Analyze step-by-step in `reasoning`. Mention if you see Trading Context vs Gambling Context.
2. Count the EXACT number of toxic posts found.
3. Output valid JSON only."""

BRAND_SAFETY_PROMPT = """Analyze these posts for brand safety.

POSTS:
{posts_text}

Output JSON:
{{"reasoning": "<Explanation identifying context (e.g., 'User discusses futures trading on MEXC, mentions liquidation. This is Crypto Trading, NOT Gambling')>", "toxic_category": "GAMBLING"|"ADULT"|"SCAM"|null, "toxic_post_count": <int>, "total_posts": <int>, "evidence": ["toxic phrase 1", "toxic phrase 2"], "severity_label": "CRITICAL"|"HIGH"|"MEDIUM"|"LOW"|"SAFE"}}

SEVERITY GUIDE (use semantic judgement, not math):
- CRITICAL: Channel exists SOLELY for this toxic topic (Dominant theme, almost every post)
- HIGH: Frequent toxic posts mixed with normal content (Regular pattern)
- MEDIUM: Occasional promotional/toxic posts (Sometimes)
- LOW: Rare mentions or borderline cases (1-2 posts)
- SAFE: No toxic content found or context is clearly educational/news"""


def analyze_brand_safety(messages: list) -> Optional[dict]:
    """
    V47.0: LLM-based Brand Safety –∞–Ω–∞–ª–∏–∑ (Grandmaster Edition).

    –£–ª—É—á—à–µ–Ω–∏—è V47.0:
    - Chain-of-Thought (reasoning field) –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    - Semantic severity (–Ω–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã, –∞ —Å–º—ã—Å–ª: "Dominant theme", "Occasional")
    - Context Traps ‚Äî —è–≤–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã SAFE —Å–ª—É—á–∞–µ–≤
    - De-obfuscation –∫–∞–∫ Core Principle

    Returns:
        dict: {
            'is_toxic': bool,
            'toxic_category': 'GAMBLING'|'ADULT'|'SCAM'|None,
            'confidence': int (0-100),
            'toxic_ratio': float,
            'severity': 'CRITICAL'|'HIGH'|'MEDIUM'|'LOW'|'SAFE',
            'evidence': list[str],
            'reasoning': str  # V47.0: CoT explanation
        }
    """
    posts_text = _prepare_posts_text(messages)

    if not posts_text or len(posts_text) < 100:
        if DEBUG_LLM_ANALYZER:
            print("LLM BrandSafety: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return {
            'is_toxic': False,
            'toxic_category': None,
            'confidence': 0,
            'toxic_ratio': 0.0,
            'severity': 'SAFE',
            'evidence': [],
            'reasoning': 'Not enough text to analyze'
        }

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è LLM
    truncated_text = posts_text[:8000]

    prompt = BRAND_SAFETY_PROMPT.format(posts_text=truncated_text)

    if DEBUG_LLM_ANALYZER:
        print(f"\n{'='*60}")
        print(f"BRAND SAFETY ANALYZER V47.0 - {len(messages)} posts")
        print(f"{'='*60}\n")

    response = _call_ollama(BRAND_SAFETY_SYSTEM, prompt)

    if not response:
        return None

    if DEBUG_LLM_ANALYZER:
        print(f"BRAND SAFETY RESPONSE:\n{response[:500]}")

    # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
    default_values = {
        "reasoning": "",
        "toxic_category": None,
        "toxic_post_count": 0,
        "total_posts": len(messages),
        "evidence": [],
        "severity_label": "SAFE"
    }
    data, warnings = safe_parse_json(response, default_values)

    if DEBUG_LLM_ANALYZER and warnings:
        print(f"JSON PARSE WARNINGS:")
        for w in warnings:
            print(f"  - {w}")

    if data:
        toxic_count = int(data.get("toxic_post_count", 0))
        total = int(data.get("total_posts", len(messages)))
        # V47.0: Python —Å—á–∏—Ç–∞–µ—Ç —Ç–æ—á–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç, –Ω–µ LLM
        toxic_ratio = toxic_count / total if total > 0 else 0.0

        category = data.get("toxic_category")
        severity = data.get("severity_label", "SAFE")
        reasoning = data.get("reasoning", "")

        # V47.0: Confidence –≤—ã—á–∏—Å–ª—è–µ–º –∏–∑ severity (LLM –±–æ–ª—å—à–µ –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ confidence)
        # CRITICAL=95, HIGH=80, MEDIUM=60, LOW=40, SAFE=0
        severity_to_confidence = {
            "CRITICAL": 95,
            "HIGH": 80,
            "MEDIUM": 60,
            "LOW": 40,
            "SAFE": 0
        }
        confidence = severity_to_confidence.get(severity, 0)

        # V47.0: is_toxic –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ severity_label
        # CRITICAL/HIGH = toxic, MEDIUM —Å ratio > 10% = toxic
        is_toxic = False
        if category and category != "null":
            if severity in ["CRITICAL", "HIGH"]:
                is_toxic = True
            elif severity == "MEDIUM" and toxic_ratio > 0.10:
                is_toxic = True

        result = {
            'is_toxic': is_toxic,
            'toxic_category': category if is_toxic else None,
            'confidence': confidence,
            'toxic_ratio': round(toxic_ratio, 3),
            'severity': severity,
            'evidence': data.get("evidence", [])[:5],  # –ú–∞–∫—Å 5 –ø—Ä–∏–º–µ—Ä–æ–≤
            'reasoning': reasoning  # V47.0: CoT explanation
        }

        if DEBUG_LLM_ANALYZER:
            print(f"LLM BrandSafety V47.0: {severity}")
            if reasoning:
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 150 —Å–∏–º–≤–æ–ª–æ–≤ reasoning
                print(f"  üí≠ Reasoning: {reasoning[:150]}...")
            if is_toxic:
                print(f"  ‚ö†Ô∏è TOXIC: {category} (ratio: {toxic_ratio:.1%})")
            else:
                print(f"  ‚úÖ SAFE")

        return result

    return None


# === COMMENT ANALYZER V40.1 ===

COMMENT_ANALYZER_SYSTEM = """You are a Turing-Test Level Comment Analyst (V45.0).
Analyze the authenticity of the audience based on their comments relative to the Post Context.

## INPUT DATA
1. **CHANNEL INFO:** Type/Category.
2. **POST CONTEXT:** The text users are reacting to.
3. **COMMENTS:** User replies.

## ANALYSIS LOGIC

### ü§ñ BOT SIGNALS (Red Flags)
1. **Language Mismatch:** English generic comments ("Great project", "Awesome", "Sir") on a Russian channel.
2. **Context Blindness:** - Post: "Bitcoin crashed, I lost everything."
   - Comment: "Great project! To the moon! üî•" (Zero context awareness).
3. **Crypto Spam:** "Airdrop", "Claim reward", "Wallet connect", Asian characters spam.
4. **Pattern Repetition:** Multiple users posting identical phrases.

### üë§ HUMAN SIGNALS (Green Flags)
1. **Context Relevance:** Comments discussing specific details from the Post Context.
2. **Technical/Slang:** Terms like "deploy", "API", "—Å–∫–∞–º", "–ª–∏–∫–≤–∏–¥", "–∏–º–±–∞".
3. **Debate/Negativity:** "–ê–≤—Ç–æ—Ä –Ω–µ –ø—Ä–∞–≤", "–≠—Ç–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç". Bots are rarely negative/argumentative.
4. **Brevity != Bot:** "+", "–°–æ–≥–ª", "–ö–µ–∫" are normal human reactions in Telegram.

Output ONLY valid JSON."""

COMMENT_ANALYZER_PROMPT_V3 = """Analyze these comments.

CHANNEL TYPE: {channel_type}
POST CONTEXT: {posts_context}
COMMENTS: {comments_text}

JSON Output:
{{"total_comments": <int>, "suspicious_bot_count": <int>, "bot_signals": ["list of specific reasons"], "authenticity_tier": "HIGH"|"MEDIUM"|"LOW", "trust_sentiment": "POSITIVE"|"NEGATIVE"|"SKEPTICAL"|"NEUTRAL"}}"""


def analyze_comments(comments: list, posts: list = None, channel_type: str = "GENERAL") -> Optional[CommentAnalysisResult]:
    """
    V40.0: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫–∞–Ω–∞–ª–∞ —Å —É—á—ë—Ç–æ–º —Ç–∏–ø–∞ –∫–∞–Ω–∞–ª–∞.

    Args:
        comments: –°–ø–∏—Å–æ–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        posts: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è sarcasm detection
        channel_type: –¢–∏–ø –∫–∞–Ω–∞–ª–∞ (TECH, CRYPTO, ENTERTAINMENT, etc.)
    """
    comments_text = _prepare_comments_text(comments)

    if not comments_text or len(comments_text) < 50:
        print("LLM CommentAnalyzer: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return None

    # V40.0: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è sarcasm detection
    posts_context = "–ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–æ–≤"
    if posts:
        posts_context = _prepare_posts_text(posts[:10])[:2000]

    # V40.0: –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç —Å channel_type
    prompt = COMMENT_ANALYZER_PROMPT_V3.format(
        channel_type=channel_type,
        posts_context=posts_context,
        comments_text=comments_text  # v40.2: –±–µ–∑ –ª–∏–º–∏—Ç–∞
    )

    if DEBUG_LLM_ANALYZER:
        print(f"\n{'='*60}")
        print(f"COMMENT ANALYZER V40.0 - {len(comments)} comments, {len(comments_text)} chars")
        print(f"Channel type: {channel_type} | Posts context: {len(posts_context)} chars")
        print(f"{'='*60}\n")

    response = _call_ollama(COMMENT_ANALYZER_SYSTEM, prompt)

    if not response:
        return None

    if DEBUG_LLM_ANALYZER:
        print(f"COMMENT ANALYZER RESPONSE:\n{response[:500]}")

    # V50.0: –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç —Å authenticity_tier
    default_values = {
        "total_comments": len(comments),
        "suspicious_bot_count": 0,
        "bot_signals": [],
        "authenticity_tier": "HIGH",
        "trust_sentiment": "NEUTRAL"
    }
    data, warnings = safe_parse_json(response, default_values)

    if DEBUG_LLM_ANALYZER and warnings:
        print(f"JSON PARSE WARNINGS:")
        for w in warnings:
            print(f"  - {w}")

    if data:
        # V50.0: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –Ω–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç –≤ —Å—Ç–∞—Ä—ã–π –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        total = int(data.get("total_comments", len(comments)))
        bot_count = int(data.get("suspicious_bot_count", 0))

        # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (bot_percentage)
        if "bot_percentage" in data:
            bot_pct = int(data.get("bot_percentage", 0))
        else:
            bot_pct = int(bot_count / total * 100) if total > 0 else 0

        # V50.0: –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º authenticity_tier –≤ trust_score
        tier = data.get("authenticity_tier", "MEDIUM")
        if "trust_score" in data:
            trust_score = int(data.get("trust_score", 50))
        else:
            trust_map = {"HIGH": 95, "MEDIUM": 70, "LOW": 30}
            trust_score = trust_map.get(tier, 50)

        # V50.0: trust_sentiment –∫–∞–∫ trust_signal
        sentiment = data.get("trust_sentiment", "NEUTRAL")
        trust_signals = data.get("trust_signals", [f"Sentiment: {sentiment}"])

        return CommentAnalysisResult(
            bot_percentage=bot_pct,
            bot_signals=data.get("bot_signals", []),
            trust_score=trust_score,
            trust_signals=trust_signals,
            raw_response=response
        )

    if DEBUG_LLM_ANALYZER:
        print(f"COMMENT ANALYZER: Failed to parse response")
    return None


# === AD PERCENTAGE ANALYZER V40.0 ===

AD_ANALYZER_SYSTEM = """You are a conservative Advertising Auditor for Telegram channels (V50.0).
Your goal is to identify paid promotional content with 100% certainty.

## CORE PHILOSOPHY
- **Conservative Approach:** If you are 99% sure it is an ad, but have 1% doubt -> IT IS NOT AN AD.
- **False Positives are fatal:** Marking a genuine review or job post as an "Ad" is a critical failure.

## CLASSIFICATION RULES

### ‚úÖ DEFINITELY AN AD (Count this)
1. **Explicit Tags:** #—Ä–µ–∫–ª–∞–º–∞, #ad, #–ø–∞—Ä—Ç–Ω—ë—Ä, #promo, "erid:".
2. **Paid Markers:** "–ù–∞ –ø—Ä–∞–≤–∞—Ö —Ä–µ–∫–ª–∞–º—ã", "–°–ø–æ–Ω—Å–æ—Ä –ø–æ—Å—Ç–∞", "–ó–∞–∫–∞–∑–∞—Ç—å —Ä–µ–∫–ª–∞–º—É –º–æ–∂–Ω–æ —Ç—É—Ç".
3. **External Push:** Aggressive sales pitch for a 3rd party product with a distinct call-to-action ("Buy now", "Subscribe here").

### ‚ùå NOT AN AD (Ignore this)
1. **Self-Promotion:** Author promoting their own course, chat, or product (Internal traffic).
2. **Job/Hiring:** "Looking for a developer", "Vacancy".
3. **Genuine Reviews:** "I tested X and liked it" (without promo codes or tracking links).
4. **Cross-promo:** "Subscribe to my friend's channel" (unless clearly paid).
5. **Donations:** Patreon, Boosty, "Support the channel".

## OUTPUT FORMAT
1. First, provide a brief **Reasoning** listing IDs of posts identified as ads.
2. Then, output strictly valid JSON."""

AD_ANALYZER_PROMPT = """Analyze the provided list of posts.
Input Format: <id> text...

POSTS:
{posts_text}

Output JSON structure:
{{"total_posts_scanned": <int>, "ad_post_count": <int>, "ad_post_ids": [<list of integers>], "reasoning": "Brief explanation of why detected posts are ads"}}"""


def analyze_ad_percentage(messages: list) -> Optional[int]:
    """
    V38.0: –õ—ë–≥–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ ad_percentage —á–µ—Ä–µ–∑ LLM.

    –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π —á–µ–º keyword-based, —Ç–∞–∫ –∫–∞–∫ –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    –ë—ã—Å—Ç—Ä–µ–µ —á–µ–º –ø–æ–ª–Ω—ã–π PostAnalyzer (5-10 —Å–µ–∫ vs 30+ —Å–µ–∫).

    Returns:
        int: –ø—Ä–æ—Ü–µ–Ω—Ç —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ (0-100) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    posts_text = _prepare_posts_text(messages)

    if not posts_text or len(posts_text) < 100:
        if DEBUG_LLM_ANALYZER:
            print("LLM AdAnalyzer: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return None

    # v41.0: –û–±—Ä–µ–∑–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —Å—á–∏—Ç–∞–µ–º –†–ï–ê–õ–¨–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤ –≤ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
    truncated_text = posts_text[:6000]
    actual_posts_count = truncated_text.count("[Post ")

    prompt = AD_ANALYZER_PROMPT.format(posts_text=truncated_text)

    if DEBUG_LLM_ANALYZER:
        print(f"\n{'='*60}")
        print(f"AD ANALYZER V40.0 - {len(messages)} posts, {len(posts_text)} chars")
        print(f"{'='*60}\n")

    response = _call_ollama(AD_ANALYZER_SYSTEM, prompt)

    if not response:
        return None

    if DEBUG_LLM_ANALYZER:
        print(f"AD ANALYZER RESPONSE:\n{response[:300]}")

    # V50.0: –ù–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç —Å ad_post_count
    default_values = {"ad_post_count": 0, "total_posts_scanned": len(messages), "ad_post_ids": []}
    data, warnings = safe_parse_json(response, default_values)

    if DEBUG_LLM_ANALYZER and warnings:
        print(f"JSON PARSE WARNINGS:")
        for w in warnings:
            print(f"  - {w}")

    if data:
        # V50.0: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –Ω–æ–≤–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ (ad_post_count) –∏ —Å—Ç–∞—Ä–æ–≥–æ (ad_count)
        ad_count = int(data.get("ad_post_count", 0) or data.get("ad_count", 0))

        # v41.0: –ò—Å–ø–æ–ª—å–∑—É–µ–º –†–ï–ê–õ–¨–ù–û–ï –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å—Ç–æ–≤, –Ω–µ —Ç–æ —á—Ç–æ LLM –ø–æ—Å—á–∏—Ç–∞–ª
        total = actual_posts_count if actual_posts_count > 0 else len(messages)

        # –ü–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
        if total > 0:
            ad_pct = int(ad_count / total * 100)
        else:
            ad_pct = 0

        if DEBUG_LLM_ANALYZER:
            print(f"LLM AdAnalyzer: {ad_pct}% —Ä–µ–∫–ª–∞–º—ã ({ad_count}/{total} –ø–æ—Å—Ç–æ–≤)")
        return max(0, min(100, ad_pct))  # Clamp 0-100

    if DEBUG_LLM_ANALYZER:
        print(f"AD ANALYZER: Failed to parse response")
    return None


# === –ì–õ–ê–í–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† ===

class LLMAnalyzer:
    """–ü–æ–ª–Ω—ã–π LLM –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞ V40.0"""

    def __init__(self):
        # v23.0: unified cache from cache.py
        self.cache = get_llm_cache()
        print(f"LLM Analyzer: Ollama ({OLLAMA_MODEL})")

    def analyze(
        self,
        channel_id: int,
        messages: list,
        comments: list,
        category: str = "DEFAULT"
    ) -> LLMAnalysisResult:
        """
        V40.0: –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞ —Å —É—á—ë—Ç–æ–º —Ç–∏–ø–∞ –∫–∞–Ω–∞–ª–∞.

        Args:
            channel_id: ID –∫–∞–Ω–∞–ª–∞
            messages: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤
            comments: –°–ø–∏—Å–æ–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∫–∞–Ω–∞–ª–∞ (–¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫)

        Returns:
            LLMAnalysisResult —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        result = LLMAnalysisResult(posts=None, comments=None, safety=None)

        # V40.0: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–∞–Ω–∞–ª–∞ –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤
        channel_type = infer_channel_type(messages, category)
        if DEBUG_LLM_ANALYZER:
            print(f"üìä Channel type detected: {channel_type} (category: {category})")

        # V46.0: Brand Safety –∞–Ω–∞–ª–∏–∑ (–ø–µ—Ä–µ–¥ Ad, —á—Ç–æ–±—ã —Å—Ä–∞–∑—É EXCLUDED –µ—Å–ª–∏ CRITICAL)
        safety_result = analyze_brand_safety(messages)
        if safety_result:
            result.safety = safety_result
            if safety_result.get('is_toxic') and safety_result.get('severity') == 'CRITICAL':
                # CRITICAL = —Å—Ä–∞–∑—É EXCLUDED, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã
                if DEBUG_LLM_ANALYZER:
                    print(f"‚õî CRITICAL TOXIC: {safety_result.get('toxic_category')} - skipping other analysis")
                result.calculate_impact_v2()
                return result

        # V40.0: Ad Percentage —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
        ad_pct = analyze_ad_percentage(messages)
        if ad_pct is not None:
            result.posts = PostAnalysisResult(
                brand_safety=100,  # –ù–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º (–±–µ—Å–ø–æ–ª–µ–∑–Ω–æ)
                toxicity=0,
                violence=0,
                military_conflict=0,
                political_quantity=0,
                political_risk=0,
                misinformation=0,
                ad_percentage=ad_pct,
                red_flags=[]
            )

        # V40.0: Comment Analyzer —Å —É—á—ë—Ç–æ–º —Ç–∏–ø–∞ –∫–∞–Ω–∞–ª–∞
        if comments and len(comments) >= 5:
            result.comments = analyze_comments(comments, posts=messages, channel_type=channel_type)
        elif DEBUG_LLM_ANALYZER:
            # v42.0: –ø–æ–¥ DEBUG —Ñ–ª–∞–≥
            print(f"LLM: –ü—Ä–æ–ø—É—Å–∫ CommentAnalyzer (–º–∞–ª–æ –∫–æ–º–º–µ–Ω—Ç–æ–≤: {len(comments) if comments else 0})")

        # V2.1: –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π impact ‚Äî —Ç–æ–ª—å–∫–æ –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        result.calculate_impact_v2()

        return result

    def save_cache(self):
        # v23.0: unified cache from cache.py (JSONCache –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç)
        pass  # JSONCache —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏ –∫–∞–∂–¥–æ–º set(), —è–≤–Ω—ã–π save –Ω–µ –Ω—É–∂–µ–Ω


# v42.0: –£–¥–∞–ª—ë–Ω –º—ë—Ä—Ç–≤—ã–π –∫–æ–¥ print_analysis_result() ‚Äî 0 –≤—ã–∑–æ–≤–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–µ
