"""
LLM –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞–Ω–∞–ª–æ–≤ v38.0

–î–≤–∞ –º–æ–¥—É–ª—è:
1. AdAnalyzer ‚Äî % —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM)
2. CommentAnalyzer ‚Äî Comment Authenticity + Trust Score (–∞–Ω–∞–ª–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤)

v38.0 –∏–∑–º–µ–Ω–µ–Ω–∏—è:
- –î–æ–±–∞–≤–ª–µ–Ω AdAnalyzer ‚Äî –ª—ë–≥–∫–∏–π –∞–Ω–∞–ª–∏–∑ ad_percentage —á–µ—Ä–µ–∑ LLM
- PostAnalyzer (toxicity, violence, political) –æ—Ç–∫–ª—é—á—ë–Ω –∫–∞–∫ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–π
- keep_alive: -1 ‚Äî –º–æ–¥–µ–ª—å –Ω–µ –≤—ã–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ –ø–∞–º—è—Ç–∏ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

–ú–µ—Ç—Ä–∏–∫–∏:
- ad_percentage: % —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ (–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ —á–µ–º keyword-based)
- authenticity: % –∂–∏–≤—ã—Ö –ª—é–¥–µ–π –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö (100 = –≤—Å–µ –∂–∏–≤—ã–µ)
- bot_percentage: % –±–æ—Ç-–ø–æ–¥–æ–±–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
- trust_score: –¥–æ–≤–µ—Ä–∏–µ –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –∫ –∫–æ–Ω—Ç–µ–Ω—Ç—É

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Ollama + Qwen3-8B
"""

import json
import asyncio
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any
from dataclasses import dataclass

import requests

# === V2.0: JSON REPAIR ===
try:
    from json_repair import repair_json
    HAS_JSON_REPAIR = True
except ImportError:
    HAS_JSON_REPAIR = False
    print("WARNING: json_repair not installed. pip install json-repair")


# === V2.0: JSON PARSING ===

def safe_parse_json(response: str, default_values: dict = None) -> tuple:
    """
    V2.0: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç LLM —Å multi-level fallback.

    Levels:
    1. Direct json.loads (find JSON object in text)
    2. json_repair library (fix trailing commas, etc.)
    3. Regex extraction of known fields

    Returns:
        (data, warnings): parsed dict and list of warnings
    """
    warnings = []

    if not response or not response.strip():
        warnings.append("Empty response from LLM")
        return default_values or {}, warnings

    # Level 1: Find and parse JSON object
    try:
        # Find balanced braces
        start_idx = response.find('{')
        if start_idx != -1:
            depth = 0
            end_idx = start_idx
            for i, char in enumerate(response[start_idx:], start_idx):
                if char == '{':
                    depth += 1
                elif char == '}':
                    depth -= 1
                    if depth == 0:
                        end_idx = i + 1
                        break

            json_candidate = response[start_idx:end_idx]
            data = json.loads(json_candidate)
            warnings.append("L1: Direct JSON parse succeeded")
            return _fill_defaults(data, default_values), warnings
    except json.JSONDecodeError as e:
        warnings.append(f"L1: JSON decode error - {e.msg}")
    except Exception as e:
        warnings.append(f"L1: Parse error - {e}")

    # Level 2: json_repair library
    if HAS_JSON_REPAIR:
        try:
            repaired = repair_json(response)
            if repaired:
                data = json.loads(repaired)
                warnings.append("L2: json_repair succeeded")
                return _fill_defaults(data, default_values), warnings
        except Exception as e:
            warnings.append(f"L2: json_repair failed - {e}")
    else:
        warnings.append("L2: json_repair not installed, skipping")

    # Level 3: Regex extraction
    try:
        data = _regex_extract_fields(response)
        if data:
            warnings.append(f"L3: Regex extracted {len(data)} fields")
            return _fill_defaults(data, default_values), warnings
    except Exception as e:
        warnings.append(f"L3: Regex extraction failed - {e}")

    warnings.append("FAILED: All parsing levels exhausted")
    return default_values or {}, warnings


def _fill_defaults(data: dict, default_values: dict) -> dict:
    """Fill missing fields with defaults."""
    if not default_values:
        return data

    for key, default in default_values.items():
        if key not in data or data[key] is None:
            data[key] = default

    return data


def _regex_extract_fields(response: str) -> dict:
    """Level 3: Extract fields via regex patterns."""
    patterns = {
        "toxicity": r'"?toxicity"?\s*[:\s]+(\d+)',
        "violence": r'"?violence"?\s*[:\s]+(\d+)',
        "military_conflict": r'"?military_conflict"?\s*[:\s]+(\d+)',
        "political_quantity": r'"?political_quantity"?\s*[:\s]+(\d+)',
        "political_risk": r'"?political_risk"?\s*[:\s]+(\d+)',
        "misinformation": r'"?misinformation"?\s*[:\s]+(\d+)',
        "ad_percentage": r'"?ad_percentage"?\s*[:\s]+(\d+)',
    }

    data = {}
    for field, pattern in patterns.items():
        match = re.search(pattern, response, re.IGNORECASE)
        if match:
            try:
                data[field] = int(match.group(1))
            except ValueError:
                pass

    # Extract red_flags array
    flags_match = re.search(r'"?red_flags"?\s*:\s*\[(.*?)\]', response, re.DOTALL)
    if flags_match:
        content = flags_match.group(1).strip()
        data["red_flags"] = re.findall(r'"([^"]+)"', content) if content else []

    return data if data else None


# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===

OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "qwen3:8b"
OLLAMA_TIMEOUT = 180  # –ë–æ–ª—å—à–µ —á–µ–º classifier ‚Äî –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–µ–µ

# –ö—ç—à
CACHE_DIR = Path(__file__).parent.parent / "cache"
LLM_CACHE_FILE = CACHE_DIR / "llm_analyzer_cache.json"
CACHE_TTL_DAYS = 7

# DEBUG
DEBUG_LLM_ANALYZER = True

# –õ–∏–º–∏—Ç—ã
MAX_POSTS_FOR_ANALYSIS = 30
MAX_COMMENTS_FOR_ANALYSIS = 999  # v40.2: –±–µ–∑ –ª–∏–º–∏—Ç–∞ (—Å–∫–æ–ª—å–∫–æ API –¥–∞—ë—Ç)
MAX_CHARS_PER_POST = 600


# === –†–ï–ó–£–õ–¨–¢–ê–¢–´ ===

@dataclass
class PostAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–æ–≤ V2.0"""
    brand_safety: int          # 0-100 (100 = –±–µ–∑–æ–ø–∞—Å–Ω–æ), –í–´–ß–ò–°–õ–Ø–ï–¢–°–Ø –í PYTHON
    toxicity: int              # 0-100 (hate speech, –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è)
    violence: int              # 0-100 (–ø—Ä–∏–∑—ã–≤—ã –∫ –Ω–∞—Å–∏–ª–∏—é, –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç)
    military_conflict: int     # 0-100 (V2.0: –≤–æ–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç violence)
    political_quantity: int    # 0-100 (% –ø–æ—Å—Ç–æ–≤ —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π)
    political_risk: int        # 0-100 (–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
    misinformation: int        # 0-100
    ad_percentage: int         # 0-100%
    red_flags: list
    raw_response: str = ""
    _brand_details: dict = None  # V2.0: calculation breakdown for debugging

    # Backwards compatibility: property –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞
    @property
    def political(self) -> int:
        """–î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç political_risk"""
        return self.political_risk


@dataclass
class CommentAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"""
    authenticity: int          # 0-100 (100 = –≤—Å–µ –∂–∏–≤—ã–µ)
    bot_percentage: int        # 0-100%
    bot_signals: list
    trust_score: int           # 0-100
    trust_signals: list
    raw_response: str = ""


@dataclass
class LLMAnalysisResult:
    """–ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç LLM –∞–Ω–∞–ª–∏–∑–∞ v37.0"""
    posts: Optional[PostAnalysisResult]
    comments: Optional[CommentAnalysisResult]

    # –†–∞—Å—á—ë—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    llm_bonus: float = 0.0           # +0-15 points
    llm_trust_factor: float = 1.0    # √ó0.15-1.0

    # v37.0: –¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
    tier: str = "PREMIUM"            # PREMIUM/STANDARD/LIMITED/RESTRICTED/EXCLUDED
    tier_cap: int = 100              # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –¥–ª—è —Ç–∏—Ä–∞
    exclusion_reason: Optional[str] = None  # –ü—Ä–∏—á–∏–Ω–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–µ—Å–ª–∏ EXCLUDED)

    # v36.1: –î–µ—Ç–∞–ª–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    _brand_mult: float = 1.0
    _comment_mult: float = 1.0
    _political_mult: float = 1.0

    def calculate_impact_v2(self):
        """
        V2.1: –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç ‚Äî —Ç–æ–ª—å–∫–æ –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
        Post Analyzer –æ—Ç–∫–ª—é—á–µ–Ω –∫–∞–∫ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–π.
        """
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        self.tier = "STANDARD"
        self.tier_cap = 85
        self.exclusion_reason = None
        self.llm_bonus = 5.0  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–æ–Ω—É—Å

        # LLM Trust Factor ‚Äî —à—Ç—Ä–∞—Ñ –æ—Ç bot_percentage
        # v40.3: –®—Ç—Ä–∞—Ñ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 15% –±–æ—Ç–æ–≤, —Ä–∞—Å—Ç—ë—Ç –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ
        comment_mult = 1.0

        if self.comments and self.comments.bot_percentage is not None:
            bot_pct = self.comments.bot_percentage
            if bot_pct <= 15:
                # –î–æ 15% –±–æ—Ç–æ–≤ ‚Äî –±–µ–∑ —à—Ç—Ä–∞—Ñ–∞
                comment_mult = 1.0
            else:
                # –û—Ç 15% –¥–æ 100% ‚Äî –ª–∏–Ω–µ–π–Ω—ã–π —à—Ç—Ä–∞—Ñ
                # 30% –±–æ—Ç–æ–≤ ‚Üí -15%, 50% ‚Üí -35%, 100% ‚Üí -70%
                penalty = (bot_pct - 15) / 100.0
                comment_mult = max(0.3, 1.0 - penalty)

        self._comment_mult = comment_mult
        self._brand_mult = 1.0
        self._political_mult = 1.0
        self.llm_trust_factor = comment_mult

        if DEBUG_LLM_ANALYZER:
            print(f"üìä V2.1: STANDARD (comment_mult={comment_mult})")


# === –ö–≠–®–ò–†–û–í–ê–ù–ò–ï ===

def _load_cache() -> dict:
    if not LLM_CACHE_FILE.exists():
        return {}
    try:
        with open(LLM_CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


def _save_cache(cache: dict):
    CACHE_DIR.mkdir(exist_ok=True)
    try:
        with open(LLM_CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"LLM Cache save error: {e}")


# === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===

def _clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Å—Å—ã–ª–æ–∫ –∏ –ª–∏—à–Ω–µ–≥–æ"""
    if not text:
        return ""
    text = re.sub(r'https?://\S+', '', text)
    text = re.sub(r't\.me/\S+', '', text)
    text = re.sub(r'[\U0001F600-\U0001F64F]{3,}', '', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def _detect_footer(posts_texts: list, min_occurrences: int = 5) -> Optional[str]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ñ—É—Ç–µ—Ä –≤ –ø–æ—Å—Ç–∞—Ö.
    –§—É—Ç–µ—Ä = —Ç–µ–∫—Å—Ç –≤ –∫–æ–Ω—Ü–µ –ø–æ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –≤ min_occurrences –ø–æ—Å—Ç–∞—Ö.
    """
    if len(posts_texts) < min_occurrences:
        return None

    # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞
    endings = []
    for text in posts_texts:
        if len(text) > 50:
            endings.append(text[-200:])

    if len(endings) < min_occurrences:
        return None

    # –ò—â–µ–º –æ–±—â–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ –∫–æ–Ω—Ü–∞—Ö
    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: –∏—â–µ–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —á–∞—Å—Ç–æ
    from collections import Counter

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ –∏—â–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –±–ª–æ–∫–∏
    line_counts = Counter()
    for ending in endings:
        lines = ending.split('\n')
        for line in lines:
            line = line.strip()
            if len(line) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                line_counts[line] += 1

    # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ >50% –ø–æ—Å—Ç–æ–≤
    threshold = len(posts_texts) * 0.4
    footer_lines = [line for line, count in line_counts.items() if count >= threshold]

    if footer_lines:
        return footer_lines[0]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—É—é —á–∞—Å—Ç—É—é
    return None


def _remove_footer(text: str, footer: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç —Ñ—É—Ç–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not footer:
        return text

    # –ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É —Å —Ñ—É—Ç–µ—Ä–æ–º –∏ –≤—Å—ë –ø–æ—Å–ª–µ –Ω–µ—ë
    idx = text.find(footer)
    if idx > 0:
        return text[:idx].strip()
    return text


def _prepare_posts_text(messages: list) -> str:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ v35.1"""
    # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
    raw_texts = []
    for msg in messages[:MAX_POSTS_FOR_ANALYSIS]:
        text = ""
        if hasattr(msg, 'message') and msg.message:
            text = msg.message
        elif hasattr(msg, 'text') and msg.text:
            text = msg.text
        if text and len(text) > 30:
            raw_texts.append(_clean_text(text))

    # –î–µ—Ç–µ–∫—Ç–∏–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ñ—É—Ç–µ—Ä
    footer = _detect_footer(raw_texts)
    if footer and DEBUG_LLM_ANALYZER:
        print(f"DETECTED FOOTER: '{footer[:50]}...'")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã –±–µ–∑ —Ñ—É—Ç–µ—Ä–∞
    posts = []
    for i, text in enumerate(raw_texts):
        clean = _remove_footer(text, footer)[:MAX_CHARS_PER_POST]
        if clean and len(clean) > 30:
            posts.append(f"[Post {i+1}]: {clean}")

    return "\n\n".join(posts)


def _prepare_comments_text(comments: list) -> str:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    texts = []
    for i, comment in enumerate(comments[:MAX_COMMENTS_FOR_ANALYSIS]):
        text = ""
        if hasattr(comment, 'message') and comment.message:
            text = comment.message
        elif hasattr(comment, 'text') and comment.text:
            text = comment.text
        elif isinstance(comment, str):
            text = comment

        if text:
            clean = _clean_text(text)[:200]
            if clean and len(clean) > 5:
                texts.append(f"[{i+1}]: {clean}")

    return "\n".join(texts)


# === OLLAMA API ===

def _call_ollama(system_prompt: str, user_message: str) -> Optional[str]:
    """–ó–∞–ø—Ä–æ—Å –∫ Ollama"""
    payload = {
        "model": OLLAMA_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ],
        "stream": False,
        "think": False,
        "keep_alive": -1,  # v38.0: –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –≤—ã–≥—Ä—É–∂–∞—Ç—å –º–æ–¥–µ–ª—å –∏–∑ –ø–∞–º—è—Ç–∏
        "options": {
            "temperature": 0.3,
            "num_predict": 500
        }
    }

    try:
        response = requests.post(OLLAMA_URL, json=payload, timeout=OLLAMA_TIMEOUT)
        if response.status_code != 200:
            print(f"OLLAMA: HTTP {response.status_code}")
            return None

        data = response.json()
        content = data.get("message", {}).get("content", "").strip()
        return content

    except requests.exceptions.ConnectionError:
        print("OLLAMA: –ù–µ –∑–∞–ø—É—â–µ–Ω! –ó–∞–ø—É—Å—Ç–∏: ollama serve")
        return None
    except requests.exceptions.Timeout:
        print(f"OLLAMA: –¢–∞–π–º–∞—É—Ç ({OLLAMA_TIMEOUT} —Å–µ–∫)")
        return None
    except Exception as e:
        print(f"OLLAMA: –û—à–∏–±–∫–∞ - {e}")
        return None


# === CHANNEL TYPE DETECTION V40.0 ===

CHANNEL_TYPE_KEYWORDS = {
    "TECH": ["python", "javascript", "–∫–æ–¥", "–ø—Ä–æ–≥—Ä–∞–º–º", "dev", "api", "github", "npm", "docker", "react", "vue", "backend", "frontend", "llm", "ml", "ai", "–Ω–µ–π—Ä–æ—Å–µ—Ç", "–º–æ–¥–µ–ª", "gpt", "ollama", "gguf"],
    "CRYPTO": ["btc", "eth", "–∫—Ä–∏–ø—Ç", "—Ç–æ–∫–µ–Ω", "defi", "nft", "–±–ª–æ–∫—á–µ–π–Ω", "–±–∏—Ç–∫–æ–∏–Ω", "—ç—Ñ–∏—Ä", "coin", "swap", "airdrop", "wallet"],
    "NEWS": ["–Ω–æ–≤–æ—Å—Ç–∏", "news", "—Å—Ä–æ—á–Ω–æ", "breaking", "–ø–æ–ª–∏—Ç–∏–∫", "—ç–∫–æ–Ω–æ–º–∏–∫", "–∏–Ω—Ñ–ª—è—Ü", "–∫—É—Ä—Å –≤–∞–ª—é—Ç"],
    "ENTERTAINMENT": ["–º–µ–º", "—é–º–æ—Ä", "–ø—Ä–∏–∫–æ–ª", "—Å–º–µ—à–Ω", "—Ñ–∏–ª—å–º", "—Å–µ—Ä–∏–∞–ª", "–∏–≥—Ä", "anime", "–∞–Ω–∏–º–µ", "–º—É–∑—ã–∫"],
    "BUSINESS": ["–±–∏–∑–Ω–µ—Å", "—Å—Ç–∞—Ä—Ç–∞–ø", "–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º", "–º–∞—Ä–∫–µ—Ç–∏–Ω–≥", "–ø—Ä–æ–¥–∞–∂", "—Ñ–∏–Ω–∞–Ω—Å", "–∏–Ω–≤–µ—Å—Ç–∏—Ü", "–∞–∫—Ü–∏"],
}


def infer_channel_type(messages: list = None, category: str = None) -> str:
    """
    V40.0: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∫–∞–Ω–∞–ª–∞ –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤.

    Args:
        messages: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤ (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º)
        category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∏–∑ classifier (TECH, AI_ML, CRYPTO, etc.)

    Returns:
        str: TECH, CRYPTO, NEWS, ENTERTAINMENT, BUSINESS, –∏–ª–∏ GENERAL
    """
    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: Explicit category from classifier
    if category:
        cat_upper = category.upper()
        if cat_upper in ["TECH", "AI_ML", "EDUCATION"]:
            return "TECH"
        if cat_upper == "CRYPTO":
            return "CRYPTO"
        if cat_upper == "NEWS":
            return "NEWS"
        if cat_upper in ["ENTERTAINMENT", "LIFESTYLE"]:
            return "ENTERTAINMENT"
        if cat_upper in ["BUSINESS", "FINANCE", "REAL_ESTATE"]:
            return "BUSINESS"

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    if not messages:
        return "GENERAL"

    # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–µ—Ä–≤—ã—Ö 20 –ø–æ—Å—Ç–æ–≤
    text_blob = ""
    for msg in messages[:20]:
        if hasattr(msg, 'message') and msg.message:
            text_blob += msg.message.lower() + " "
        elif hasattr(msg, 'text') and msg.text:
            text_blob += msg.text.lower() + " "

    if not text_blob:
        return "GENERAL"

    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ —Ç–∏–ø–∞–º
    scores = {}
    for ctype, keywords in CHANNEL_TYPE_KEYWORDS.items():
        scores[ctype] = sum(1 for kw in keywords if kw in text_blob)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∏–ø —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º score (–µ—Å–ª–∏ >= 3 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π)
    if scores:
        best_type = max(scores, key=scores.get)
        if scores[best_type] >= 3:
            return best_type

    return "GENERAL"


# === COMMENT ANALYZER V40.1 ===

COMMENT_ANALYZER_SYSTEM = """You are a comment authenticity analyzer V40.1.
Your goal is ACCURATE bot detection. Most Telegram channels have 0-5% bots.

## CRITICAL: 0% BOTS IS NORMAL!
Healthy channels typically have ZERO bots. Only count as bot if you see CLEAR evidence.
Do NOT inflate bot_percentage "just in case" - that creates false positives.

## WHAT MAKES A COMMENT HUMAN (NOT bot):

1. TECHNICAL TERMS = 100% HUMAN
   "gguf", "npm", "API", "llama.cpp", version numbers, library names
   ‚Üí Bots cannot generate domain-specific knowledge

2. EMOTION = 100% HUMAN
   Profanity, frustration, sarcasm, arguments, memes, slang
   "–ó–ê–ï–ë–ê–õ–°–Ø", "–Ω–∏–∏–ø—ë—Ç", "–≥–æ—Ä–∏—à—å", "—Å–∞–¥–∏—Å—å, –¥–≤–∞" ‚Üí definitely human

3. CONVERSATION = 100% HUMAN
   Back-and-forth dialogue, follow-up questions, corrections
   "–ñ–¥—É gguf" ‚Üí "–í—ã–ª–æ–∂–∏–ª–∏" ‚Üí "–∫–∞—á–∞—é" = real users talking

4. SHORT ‚â† BOT
   "works", "+1", "—Å–ø–∞—Å–∏–±–æ", "–≥–≥—É—Ñ –Ω—É–∂–µ–Ω)" are NORMAL human replies

## WHAT MAKES A COMMENT BOT:

Count as BOT if you see these patterns:
- IDENTICAL text from multiple users (copy-paste)
- Generic English praise on Russian channel ("Great post!", "Amazing!")
- Promotional spam unrelated to channel topic
- Motivational quotes with no connection to content

## CRYPTO SPAM = ALWAYS BOT (very important!):
- Airdrop spam: "–ö–ª–µ–π–º–∏–º –ê–∏—Ä–¥—Ä–æ–ø", "–¥—Ä–æ–ø –æ—Ç", "–∫–ª–µ–π–º —Ç–æ–∫–µ–Ω–∞", "claim", "airdrop"
- Korean/Chinese spam on Russian channel (ÏóêÏñ¥ÎìúÎûç, ÎìúÎ°≠, ÌÅ¥Î†àÏûÑ)
- Phishing links: random domains with /claim, /airdrop, /reward
- Link-only comments (just URL, no context)
- "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—à–µ–ª—ë–∫", "check your wallet" spam

Output ONLY valid JSON."""

COMMENT_ANALYZER_PROMPT_V3 = """Analyze these Telegram comments for bot detection.

## CHANNEL TYPE: {channel_type}

## POST CONTEXT:
{posts_context}

## COMMENTS:
{comments_text}

---

## YOUR TASK:

Count how many comments are CLEARLY bots vs humans.

### HUMAN indicators (count as REAL - authenticity +1):
- Any technical jargon or domain knowledge
- Profanity, emotion, sarcasm, slang
- Questions about the post content
- Debates, disagreements, corrections
- Personal experience ("—è –ø—Ä–æ–±–æ–≤–∞–ª", "—É –º–µ–Ω—è —Ä–∞–±–æ—Ç–∞–µ—Ç")
- Conversation flow (replies to each other)
- Short but contextually relevant ("works", "+1", "—Å–ø–∞—Å–∏–±–æ")

### BOT indicators (count as BOT):
- IDENTICAL text from different users (copy-paste)
- Generic English on Russian channel ("Great!", "Amazing!")
- Completely off-topic spam
- Suspiciously formal language
- CRYPTO SPAM (count ALL as bots!):
  * Airdrop messages: "–ö–ª–µ–π–º–∏–º –ê–∏—Ä–¥—Ä–æ–ø", "–¥—Ä–æ–ø –æ—Ç", "–∫–ª–µ–π–º —Ç–æ–∫–µ–Ω–∞"
  * Korean/Chinese text on Russian channel (ÏóêÏñ¥ÎìúÎûç, ÎìúÎ°≠)
  * Phishing links: domains with /claim, /airdrop, /reward
  * Link-only comments (just URL without context)
  * "–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—à–µ–ª—ë–∫" wallet check spam

## CALIBRATION:

IMPORTANT: Most healthy channels have 0-5% bots!
- If all comments have personality/context ‚Üí bot_percentage = 0%
- If 1-2 generic comments in 50 ‚Üí bot_percentage = 2-4%
- If 5+ identical/spam comments ‚Üí bot_percentage = 10%+

Only high bot_percentage (>10%) if you see MULTIPLE clear bot patterns.

## EXAMPLES:

Channel with 50 comments, all have technical terms or emotion:
‚Üí bot_percentage = 0%, authenticity = 100%

Channel with 50 comments, 2 say just "üëç" on technical post:
‚Üí bot_percentage = 4%, authenticity = 96%

Channel with 50 comments, 10 are identical "–û—Ç–ª–∏—á–Ω—ã–π –ø–æ—Å—Ç!":
‚Üí bot_percentage = 20%, authenticity = 80%

Output JSON:
{{"authenticity": <0-100>, "bot_percentage": <0-100>, "bot_signals": [<patterns found>], "trust_score": <0-100>, "trust_signals": [<positive signals>], "sarcasm_warning": <true/false>}}"""


def analyze_comments(comments: list, posts: list = None, channel_type: str = "GENERAL") -> Optional[CommentAnalysisResult]:
    """
    V40.0: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫–∞–Ω–∞–ª–∞ —Å —É—á—ë—Ç–æ–º —Ç–∏–ø–∞ –∫–∞–Ω–∞–ª–∞.

    Args:
        comments: –°–ø–∏—Å–æ–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        posts: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è sarcasm detection
        channel_type: –¢–∏–ø –∫–∞–Ω–∞–ª–∞ (TECH, CRYPTO, ENTERTAINMENT, etc.)
    """
    comments_text = _prepare_comments_text(comments)

    if not comments_text or len(comments_text) < 50:
        print("LLM CommentAnalyzer: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return None

    # V40.0: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è sarcasm detection
    posts_context = "–ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–æ–≤"
    if posts:
        posts_context = _prepare_posts_text(posts[:10])[:2000]

    # V40.0: –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç —Å channel_type
    prompt = COMMENT_ANALYZER_PROMPT_V3.format(
        channel_type=channel_type,
        posts_context=posts_context,
        comments_text=comments_text  # v40.2: –±–µ–∑ –ª–∏–º–∏—Ç–∞
    )

    if DEBUG_LLM_ANALYZER:
        print(f"\n{'='*60}")
        print(f"COMMENT ANALYZER V40.0 - {len(comments)} comments, {len(comments_text)} chars")
        print(f"Channel type: {channel_type} | Posts context: {len(posts_context)} chars")
        print(f"{'='*60}\n")

    response = _call_ollama(COMMENT_ANALYZER_SYSTEM, prompt)

    if not response:
        return None

    if DEBUG_LLM_ANALYZER:
        print(f"COMMENT ANALYZER RESPONSE:\n{response[:500]}")

    # V2.0: Use safe_parse_json with fallback
    default_values = {
        "authenticity": 50,
        "bot_percentage": 50,
        "bot_signals": [],
        "trust_score": 50,
        "trust_signals": [],
        "sarcasm_warning": False
    }
    data, warnings = safe_parse_json(response, default_values)

    if DEBUG_LLM_ANALYZER and warnings:
        print(f"JSON PARSE WARNINGS:")
        for w in warnings:
            print(f"  - {w}")

    if data:
        return CommentAnalysisResult(
            authenticity=int(data.get("authenticity", 50)),
            bot_percentage=int(data.get("bot_percentage", 50)),
            bot_signals=data.get("bot_signals", []),
            trust_score=int(data.get("trust_score", 50)),
            trust_signals=data.get("trust_signals", []),
            raw_response=response
        )

    print(f"COMMENT ANALYZER: Failed to parse response")
    return None


# === AD PERCENTAGE ANALYZER V40.0 ===

AD_ANALYZER_SYSTEM = """You are a Telegram advertising analyst V40.0.
Your goal is ACCURATE classification, not maximum ad detection.
CRITICAL: Distinguish between THIRD-PARTY ADS and AUTHOR'S OWN CONTENT.
When uncertain, default to NOT counting as ad.
Output ONLY valid JSON, no other text."""

AD_ANALYZER_PROMPT = """Analyze advertising content in this Telegram channel.

POSTS:
{posts_text}

---

## STEP 1: DETECT CHANNEL TYPE (do this FIRST!)

Look at ALL posts and determine what kind of channel this is:
- ARTIST/CREATOR: Posts about paintings, drawings, music, designs, handmade items
- DEVELOPER: Posts about code, projects, tools, tutorials
- BLOGGER: Personal stories, opinions, lifestyle content
- NEWS: Reposts, aggregated content from other sources
- COMPANY: Official brand channel

‚ö†Ô∏è CRITICAL RULE: If channel is ARTIST/CREATOR type:
- Posts selling their own artwork (auctions, prices, "—Å—Ç–∞–≤–∫–∏") = NOT ADS (0%)
- Posts about their own creative process = NOT ADS
- Links to their own store/gallery = NOT ADS
- ONLY count as AD if they promote SOMEONE ELSE's products

## STEP 2: COUNT ONLY THIRD-PARTY ADVERTISING

### COUNT AS AD (promoting OTHER people's stuff):
- Posts marked #—Ä–µ–∫–ª–∞–º–∞, #–ø–∞—Ä—Ç–Ω—ë—Ä, #ad, #sponsored
- Promotions of OTHER channels (not author's own)
- Affiliate links for EXTERNAL products (?ref=, promo codes)
- Crypto shills: token contracts (0x...), "fair launch"
- Paid partnerships with external brands

### NEVER COUNT AS AD (author's own content):
- Author selling THEIR OWN products (art, courses, services)
- Auctions for author's own work ("–∞—É–∫—Ü–∏–æ–Ω", "—Å—Ç–∞–≤–∫–∏", "–ª–æ—Ç")
- Author's monetization: Boosty, Patreon, Ko-fi, –¥–æ–Ω–∞—Ç—ã
- Author's other channels/platforms
- Tool mentions without affiliate context
- Personal reviews without payment disclosure

## EXAMPLES:

ARTIST CHANNEL posting "–ê—É–∫—Ü–∏–æ–Ω! –ö–∞—Ä—Ç–∏–Ω–∞ '–ó–∞–∫–∞—Ç'. –°—Ç–∞—Ä—Ç 5000‚ÇΩ. –°—Ç–∞–≤–∫–∏ –≤ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è—Ö üëá"
‚Üí This is the artist selling THEIR OWN painting
‚Üí NOT AN AD (ad_count = 0)

TECH CHANNEL posting "–†–µ–∫–æ–º–µ–Ω–¥—É—é –∫—É—Ä—Å –æ—Ç @other_channel, –ø—Ä–æ–º–æ–∫–æ–¥ SAVE20"
‚Üí This promotes ANOTHER channel with promo code
‚Üí THIS IS AN AD (ad_count = 1)

BLOGGER posting "–ú–æ–π –Ω–æ–≤—ã–π –∫—É—Ä—Å –Ω–∞ Boosty —É–∂–µ –¥–æ—Å—Ç—É–ø–µ–Ω!"
‚Üí Author's own monetization
‚Üí NOT AN AD (ad_count = 0)

## CALIBRATION:
- If channel sells author's own products ‚Üí ad_percentage should be LOW (0-10%)
- Only count THIRD-PARTY paid promotions
- When uncertain ‚Üí default to NOT AD

Output JSON: {{"channel_type": "<artist|developer|blogger|news|company|unknown>", "ad_count": <number>, "monetization_count": <number>, "total_posts": <number>, "ad_percentage": <0-100>}}"""


def analyze_ad_percentage(messages: list) -> Optional[int]:
    """
    V38.0: –õ—ë–≥–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ ad_percentage —á–µ—Ä–µ–∑ LLM.

    –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π —á–µ–º keyword-based, —Ç–∞–∫ –∫–∞–∫ –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç.
    –ë—ã—Å—Ç—Ä–µ–µ —á–µ–º –ø–æ–ª–Ω—ã–π PostAnalyzer (5-10 —Å–µ–∫ vs 30+ —Å–µ–∫).

    Returns:
        int: –ø—Ä–æ—Ü–µ–Ω—Ç —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤ (0-100) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    posts_text = _prepare_posts_text(messages)

    if not posts_text or len(posts_text) < 100:
        print("LLM AdAnalyzer: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return None

    prompt = AD_ANALYZER_PROMPT.format(posts_text=posts_text[:6000])

    if DEBUG_LLM_ANALYZER:
        print(f"\n{'='*60}")
        print(f"AD ANALYZER V40.0 - {len(messages)} posts, {len(posts_text)} chars")
        print(f"{'='*60}\n")

    response = _call_ollama(AD_ANALYZER_SYSTEM, prompt)

    if not response:
        return None

    if DEBUG_LLM_ANALYZER:
        print(f"AD ANALYZER RESPONSE:\n{response[:300]}")

    # –ü–∞—Ä—Å–∏–º JSON
    default_values = {"ad_count": 0, "total_posts": len(messages), "ad_percentage": 0}
    data, warnings = safe_parse_json(response, default_values)

    if DEBUG_LLM_ANALYZER and warnings:
        print(f"JSON PARSE WARNINGS:")
        for w in warnings:
            print(f"  - {w}")

    if data:
        ad_pct = int(data.get("ad_percentage", 0))
        ad_count = int(data.get("ad_count", 0))
        total = int(data.get("total_posts", len(messages)))

        # –í–∞–ª–∏–¥–∞—Ü–∏—è: –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º –µ—Å–ª–∏ LLM –¥–∞–ª count/total
        if ad_count > 0 and total > 0:
            calculated_pct = int(ad_count / total * 100)
            # –ï—Å–ª–∏ LLM –¥–∞–ª –ø—Ä–æ—Ü–µ–Ω—Ç —Å–∏–ª—å–Ω–æ –æ—Ç–ª–∏—á–∞—é—â–∏–π—Å—è –æ—Ç –ø–æ–¥—Å—á—ë—Ç–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥—Å—á—ë—Ç
            if abs(ad_pct - calculated_pct) > 15:
                ad_pct = calculated_pct

        print(f"LLM AdAnalyzer: {ad_pct}% —Ä–µ–∫–ª–∞–º—ã ({ad_count}/{total} –ø–æ—Å—Ç–æ–≤)")
        return max(0, min(100, ad_pct))  # Clamp 0-100

    print(f"AD ANALYZER: Failed to parse response")
    return None


# === –ì–õ–ê–í–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† ===

class LLMAnalyzer:
    """–ü–æ–ª–Ω—ã–π LLM –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞ V40.0"""

    def __init__(self):
        self.cache = _load_cache()
        print(f"LLM ANALYZER v40.0: Ollama ({OLLAMA_MODEL})")

    def analyze(
        self,
        channel_id: int,
        messages: list,
        comments: list,
        category: str = "DEFAULT"
    ) -> LLMAnalysisResult:
        """
        V40.0: –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞ —Å —É—á—ë—Ç–æ–º —Ç–∏–ø–∞ –∫–∞–Ω–∞–ª–∞.

        Args:
            channel_id: ID –∫–∞–Ω–∞–ª–∞
            messages: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤
            comments: –°–ø–∏—Å–æ–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∫–∞–Ω–∞–ª–∞ (–¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫)

        Returns:
            LLMAnalysisResult —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        result = LLMAnalysisResult(posts=None, comments=None)

        # V40.0: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∫–∞–Ω–∞–ª–∞ –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –ø—Ä–æ–º–ø—Ç–æ–≤
        channel_type = infer_channel_type(messages, category)
        if DEBUG_LLM_ANALYZER:
            print(f"üìä Channel type detected: {channel_type} (category: {category})")

        # V40.0: Ad Percentage —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
        ad_pct = analyze_ad_percentage(messages)
        if ad_pct is not None:
            result.posts = PostAnalysisResult(
                brand_safety=100,  # –ù–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º (–±–µ—Å–ø–æ–ª–µ–∑–Ω–æ)
                toxicity=0,
                violence=0,
                military_conflict=0,
                political_quantity=0,
                political_risk=0,
                misinformation=0,
                ad_percentage=ad_pct,
                red_flags=[]
            )

        # V40.0: Comment Analyzer —Å —É—á—ë—Ç–æ–º —Ç–∏–ø–∞ –∫–∞–Ω–∞–ª–∞
        if comments and len(comments) >= 5:
            result.comments = analyze_comments(comments, posts=messages, channel_type=channel_type)
        else:
            print(f"LLM: –ü—Ä–æ–ø—É—Å–∫ CommentAnalyzer (–º–∞–ª–æ –∫–æ–º–º–µ–Ω—Ç–æ–≤: {len(comments) if comments else 0})")

        # V2.1: –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π impact ‚Äî —Ç–æ–ª—å–∫–æ –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        result.calculate_impact_v2()

        return result

    def save_cache(self):
        _save_cache(self.cache)


# === –¢–ï–°–¢–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø ===

def print_analysis_result(result: LLMAnalysisResult, channel_name: str = ""):
    """–ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ v37.0"""
    print(f"\n{'='*60}")
    print(f"LLM ANALYSIS v37.0: {channel_name}")
    print(f"{'='*60}")

    # v37.0: –¢–∏—Ä –∏ cap
    print(f"\nüìä SUITABILITY TIER: {result.tier} (cap={result.tier_cap})")
    if result.exclusion_reason:
        print(f"   ‚õî EXCLUDED: {result.exclusion_reason}")

    if result.posts:
        p = result.posts
        print(f"\nüìù POST ANALYSIS:")
        print(f"   Brand Safety: {p.brand_safety}/100")
        print(f"   - Toxicity: {p.toxicity}")
        print(f"   - Violence: {p.violence}")  # v37.0
        print(f"   - Political Quantity: {p.political_quantity}%")  # v37.0
        print(f"   - Political Risk: {p.political_risk}")  # v37.0
        print(f"   - Misinformation: {p.misinformation}")
        print(f"   Ad Percentage: {p.ad_percentage}%")
        if p.red_flags:
            print(f"   Red Flags: {p.red_flags}")
    else:
        print(f"\nüìù POST ANALYSIS: –ü—Ä–æ–ø—É—â–µ–Ω (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)")

    if result.comments:
        c = result.comments
        print(f"\nüí¨ COMMENT ANALYSIS:")
        print(f"   Authenticity: {c.authenticity}/100 ({100-c.bot_percentage}% –∂–∏–≤—ã–µ)")
        print(f"   Bot Signals: {c.bot_signals}")
        print(f"   Trust Score: {c.trust_score}/100")
        print(f"   Trust Signals: {c.trust_signals}")
    else:
        print(f"\nüí¨ COMMENT ANALYSIS: –ü—Ä–æ–ø—É—â–µ–Ω (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)")

    print(f"\nüìä IMPACT ON SCORE:")
    print(f"   LLM Bonus: +{result.llm_bonus:.1f} points")
    print(f"   LLM Trust Factor: √ó{result.llm_trust_factor:.2f}")
    print(f"   Tier Cap: {result.tier_cap}")

    # –ü—Ä–∏–º–µ—Ä –≤–ª–∏—è–Ω–∏—è —Å tier cap
    example_raw = 70
    example_trust = 0.95
    old_score = example_raw * example_trust
    base_new = (example_raw + result.llm_bonus) * example_trust * result.llm_trust_factor
    new_score = min(base_new, result.tier_cap)  # v37.0: –ø—Ä–∏–º–µ–Ω—è–µ–º cap
    print(f"\n   Example: Raw=70, Trust=0.95")
    print(f"   Old formula: {old_score:.1f}")
    print(f"   New formula: {base_new:.1f} ‚Üí capped to {new_score:.1f}")

    print(f"{'='*60}\n")
