"""
LLM –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞–Ω–∞–ª–æ–≤ v37.2

–î–≤–∞ –º–æ–¥—É–ª—è:
1. PostAnalyzer ‚Äî Brand Safety + Ad Saturation (–∞–Ω–∞–ª–∏–∑ –ø–æ—Å—Ç–æ–≤)
2. CommentAnalyzer ‚Äî Comment Authenticity + Trust Score (–∞–Ω–∞–ª–∏–∑ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤)

v37.2 –∏–∑–º–µ–Ω–µ–Ω–∏—è:
- political_risk –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É–≤–µ–ª–∏—á–µ–Ω —Å 0.3 –¥–æ 0.5 (–ø–æ–ª–∏—Ç–∏–∫–∞ = 2-—è —É–≥—Ä–æ–∑–∞ –ø–æ—Å–ª–µ –Ω–∞—Å–∏–ª–∏—è)
- ADULT –∫–∞—Ç–µ–≥–æ—Ä–∏—è ‚Üí RESTRICTED tier (–∑–∞—â–∏—Ç–∞)
- Brand Safety —Ñ–æ—Ä–º—É–ª–∞: 100 - max(tox√ó0.5, violence√ó0.6, pol_risk√ó0.5, mis√ó0.4)

v37.0 –∏–∑–º–µ–Ω–µ–Ω–∏—è (–ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã –æ—Ü–µ–Ω–∫–∏):
- –¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ ‚Äî Exclusion ‚Üí Tier ‚Üí Score
- political_quantity (% –∫–æ–Ω—Ç–µ–Ω—Ç–∞) + political_risk (–æ–ø–∞—Å–Ω–æ—Å—Ç—å) ‚Äî 2D –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
- violence –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç toxicity (—Ä–∞–∑–Ω—ã–µ Floor Levels –ø–æ GARM)
- 5 —Ç–∏—Ä–æ–≤: PREMIUM/STANDARD/LIMITED/RESTRICTED/EXCLUDED —Å caps
- Floor Level exclusions (violence‚â•50, toxicity‚â•70, political_risk‚â•80)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Ollama + Qwen3-8B
"""

import json
import asyncio
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Dict, Any
from dataclasses import dataclass

import requests

# === V2.0: JSON REPAIR ===
try:
    from json_repair import repair_json
    HAS_JSON_REPAIR = True
except ImportError:
    HAS_JSON_REPAIR = False
    print("WARNING: json_repair not installed. pip install json-repair")


# === V2.0: JSON PARSING ===

def safe_parse_json(response: str, default_values: dict = None) -> tuple:
    """
    V2.0: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç LLM —Å multi-level fallback.

    Levels:
    1. Direct json.loads (find JSON object in text)
    2. json_repair library (fix trailing commas, etc.)
    3. Regex extraction of known fields

    Returns:
        (data, warnings): parsed dict and list of warnings
    """
    warnings = []

    if not response or not response.strip():
        warnings.append("Empty response from LLM")
        return default_values or {}, warnings

    # Level 1: Find and parse JSON object
    try:
        # Find balanced braces
        start_idx = response.find('{')
        if start_idx != -1:
            depth = 0
            end_idx = start_idx
            for i, char in enumerate(response[start_idx:], start_idx):
                if char == '{':
                    depth += 1
                elif char == '}':
                    depth -= 1
                    if depth == 0:
                        end_idx = i + 1
                        break

            json_candidate = response[start_idx:end_idx]
            data = json.loads(json_candidate)
            warnings.append("L1: Direct JSON parse succeeded")
            return _fill_defaults(data, default_values), warnings
    except json.JSONDecodeError as e:
        warnings.append(f"L1: JSON decode error - {e.msg}")
    except Exception as e:
        warnings.append(f"L1: Parse error - {e}")

    # Level 2: json_repair library
    if HAS_JSON_REPAIR:
        try:
            repaired = repair_json(response)
            if repaired:
                data = json.loads(repaired)
                warnings.append("L2: json_repair succeeded")
                return _fill_defaults(data, default_values), warnings
        except Exception as e:
            warnings.append(f"L2: json_repair failed - {e}")
    else:
        warnings.append("L2: json_repair not installed, skipping")

    # Level 3: Regex extraction
    try:
        data = _regex_extract_fields(response)
        if data:
            warnings.append(f"L3: Regex extracted {len(data)} fields")
            return _fill_defaults(data, default_values), warnings
    except Exception as e:
        warnings.append(f"L3: Regex extraction failed - {e}")

    warnings.append("FAILED: All parsing levels exhausted")
    return default_values or {}, warnings


def _fill_defaults(data: dict, default_values: dict) -> dict:
    """Fill missing fields with defaults."""
    if not default_values:
        return data

    for key, default in default_values.items():
        if key not in data or data[key] is None:
            data[key] = default

    return data


def _regex_extract_fields(response: str) -> dict:
    """Level 3: Extract fields via regex patterns."""
    patterns = {
        "toxicity": r'"?toxicity"?\s*[:\s]+(\d+)',
        "violence": r'"?violence"?\s*[:\s]+(\d+)',
        "military_conflict": r'"?military_conflict"?\s*[:\s]+(\d+)',
        "political_quantity": r'"?political_quantity"?\s*[:\s]+(\d+)',
        "political_risk": r'"?political_risk"?\s*[:\s]+(\d+)',
        "misinformation": r'"?misinformation"?\s*[:\s]+(\d+)',
        "ad_percentage": r'"?ad_percentage"?\s*[:\s]+(\d+)',
    }

    data = {}
    for field, pattern in patterns.items():
        match = re.search(pattern, response, re.IGNORECASE)
        if match:
            try:
                data[field] = int(match.group(1))
            except ValueError:
                pass

    # Extract red_flags array
    flags_match = re.search(r'"?red_flags"?\s*:\s*\[(.*?)\]', response, re.DOTALL)
    if flags_match:
        content = flags_match.group(1).strip()
        data["red_flags"] = re.findall(r'"([^"]+)"', content) if content else []

    return data if data else None


# === –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===

OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "qwen3:8b"
OLLAMA_TIMEOUT = 180  # –ë–æ–ª—å—à–µ —á–µ–º classifier ‚Äî –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–µ–µ

# –ö—ç—à
CACHE_DIR = Path(__file__).parent.parent / "cache"
LLM_CACHE_FILE = CACHE_DIR / "llm_analyzer_cache.json"
CACHE_TTL_DAYS = 7

# DEBUG
DEBUG_LLM_ANALYZER = True

# –õ–∏–º–∏—Ç—ã
MAX_POSTS_FOR_ANALYSIS = 30
MAX_COMMENTS_FOR_ANALYSIS = 50
MAX_CHARS_PER_POST = 600


# === –†–ï–ó–£–õ–¨–¢–ê–¢–´ ===

@dataclass
class PostAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å—Ç–æ–≤ V2.0"""
    brand_safety: int          # 0-100 (100 = –±–µ–∑–æ–ø–∞—Å–Ω–æ), –í–´–ß–ò–°–õ–Ø–ï–¢–°–Ø –í PYTHON
    toxicity: int              # 0-100 (hate speech, –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è)
    violence: int              # 0-100 (–ø—Ä–∏–∑—ã–≤—ã –∫ –Ω–∞—Å–∏–ª–∏—é, –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç)
    military_conflict: int     # 0-100 (V2.0: –≤–æ–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç, –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç violence)
    political_quantity: int    # 0-100 (% –ø–æ—Å—Ç–æ–≤ —Å –ø–æ–ª–∏—Ç–∏–∫–æ–π)
    political_risk: int        # 0-100 (–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞)
    misinformation: int        # 0-100
    ad_percentage: int         # 0-100%
    red_flags: list
    raw_response: str = ""
    _brand_details: dict = None  # V2.0: calculation breakdown for debugging

    # Backwards compatibility: property –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ –∫–æ–¥–∞
    @property
    def political(self) -> int:
        """–î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏: –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç political_risk"""
        return self.political_risk


@dataclass
class CommentAnalysisResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"""
    authenticity: int          # 0-100 (100 = –≤—Å–µ –∂–∏–≤—ã–µ)
    bot_percentage: int        # 0-100%
    bot_signals: list
    trust_score: int           # 0-100
    trust_signals: list
    raw_response: str = ""


@dataclass
class LLMAnalysisResult:
    """–ü–æ–ª–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç LLM –∞–Ω–∞–ª–∏–∑–∞ v37.0"""
    posts: Optional[PostAnalysisResult]
    comments: Optional[CommentAnalysisResult]

    # –†–∞—Å—á—ë—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    llm_bonus: float = 0.0           # +0-15 points
    llm_trust_factor: float = 1.0    # √ó0.15-1.0

    # v37.0: –¢—Ä—ë—Ö—ç—Ç–∞–ø–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
    tier: str = "PREMIUM"            # PREMIUM/STANDARD/LIMITED/RESTRICTED/EXCLUDED
    tier_cap: int = 100              # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä –¥–ª—è —Ç–∏—Ä–∞
    exclusion_reason: Optional[str] = None  # –ü—Ä–∏—á–∏–Ω–∞ –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–µ—Å–ª–∏ EXCLUDED)

    # v36.1: –î–µ—Ç–∞–ª–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    _brand_mult: float = 1.0
    _comment_mult: float = 1.0
    _political_mult: float = 1.0

    def calculate_impact_v2(self):
        """
        V2.1: –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Ä–∞—Å—á—ë—Ç ‚Äî —Ç–æ–ª—å–∫–æ –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
        Post Analyzer –æ—Ç–∫–ª—é—á–µ–Ω –∫–∞–∫ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–π.
        """
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        self.tier = "STANDARD"
        self.tier_cap = 85
        self.exclusion_reason = None
        self.llm_bonus = 5.0  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–æ–Ω—É—Å

        # LLM Trust Factor ‚Äî —Ç–æ–ª—å–∫–æ –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        comment_mult = 1.0

        if self.comments and self.comments.authenticity:
            auth = self.comments.authenticity
            if auth >= 80:
                comment_mult = 1.0
            elif auth >= 60:
                comment_mult = 0.9
            elif auth >= 40:
                comment_mult = 0.7
            elif auth >= 20:
                comment_mult = 0.5
            else:
                comment_mult = 0.3

        self._comment_mult = comment_mult
        self._brand_mult = 1.0
        self._political_mult = 1.0
        self.llm_trust_factor = comment_mult

        if DEBUG_LLM_ANALYZER:
            print(f"üìä V2.1: STANDARD (comment_mult={comment_mult})")


# === –ö–≠–®–ò–†–û–í–ê–ù–ò–ï ===

def _load_cache() -> dict:
    if not LLM_CACHE_FILE.exists():
        return {}
    try:
        with open(LLM_CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}


def _save_cache(cache: dict):
    CACHE_DIR.mkdir(exist_ok=True)
    try:
        with open(LLM_CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"LLM Cache save error: {e}")


# === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===

def _clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç —Å—Å—ã–ª–æ–∫ –∏ –ª–∏—à–Ω–µ–≥–æ"""
    if not text:
        return ""
    text = re.sub(r'https?://\S+', '', text)
    text = re.sub(r't\.me/\S+', '', text)
    text = re.sub(r'[\U0001F600-\U0001F64F]{3,}', '', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def _detect_footer(posts_texts: list, min_occurrences: int = 5) -> Optional[str]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ñ—É—Ç–µ—Ä –≤ –ø–æ—Å—Ç–∞—Ö.
    –§—É—Ç–µ—Ä = —Ç–µ–∫—Å—Ç –≤ –∫–æ–Ω—Ü–µ –ø–æ—Å—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –≤ min_occurrences –ø–æ—Å—Ç–∞—Ö.
    """
    if len(posts_texts) < min_occurrences:
        return None

    # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ –ø–æ—Å—Ç–∞
    endings = []
    for text in posts_texts:
        if len(text) > 50:
            endings.append(text[-200:])

    if len(endings) < min_occurrences:
        return None

    # –ò—â–µ–º –æ–±—â–∏–µ –ø–æ–¥—Å—Ç—Ä–æ–∫–∏ –≤ –∫–æ–Ω—Ü–∞—Ö
    # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–¥—Ö–æ–¥: –∏—â–µ–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è —á–∞—Å—Ç–æ
    from collections import Counter

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –∏ –∏—â–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –±–ª–æ–∫–∏
    line_counts = Counter()
    for ending in endings:
        lines = ending.split('\n')
        for line in lines:
            line = line.strip()
            if len(line) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                line_counts[line] += 1

    # –ù–∞—Ö–æ–¥–∏–º —Å—Ç—Ä–æ–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤ >50% –ø–æ—Å—Ç–æ–≤
    threshold = len(posts_texts) * 0.4
    footer_lines = [line for line, count in line_counts.items() if count >= threshold]

    if footer_lines:
        return footer_lines[0]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—É—é —á–∞—Å—Ç—É—é
    return None


def _remove_footer(text: str, footer: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç —Ñ—É—Ç–µ—Ä –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not footer:
        return text

    # –ü—Ä–æ–±—É–µ–º —É–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫—É —Å —Ñ—É—Ç–µ—Ä–æ–º –∏ –≤—Å—ë –ø–æ—Å–ª–µ –Ω–µ—ë
    idx = text.find(footer)
    if idx > 0:
        return text[:idx].strip()
    return text


def _prepare_posts_text(messages: list) -> str:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ v35.1"""
    # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
    raw_texts = []
    for msg in messages[:MAX_POSTS_FOR_ANALYSIS]:
        text = ""
        if hasattr(msg, 'message') and msg.message:
            text = msg.message
        elif hasattr(msg, 'text') and msg.text:
            text = msg.text
        if text and len(text) > 30:
            raw_texts.append(_clean_text(text))

    # –î–µ—Ç–µ–∫—Ç–∏–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–π—Å—è —Ñ—É—Ç–µ—Ä
    footer = _detect_footer(raw_texts)
    if footer and DEBUG_LLM_ANALYZER:
        print(f"DETECTED FOOTER: '{footer[:50]}...'")

    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ—Å—Ç—ã –±–µ–∑ —Ñ—É—Ç–µ—Ä–∞
    posts = []
    for i, text in enumerate(raw_texts):
        clean = _remove_footer(text, footer)[:MAX_CHARS_PER_POST]
        if clean and len(clean) > 30:
            posts.append(f"[Post {i+1}]: {clean}")

    return "\n\n".join(posts)


def _prepare_comments_text(comments: list) -> str:
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    texts = []
    for i, comment in enumerate(comments[:MAX_COMMENTS_FOR_ANALYSIS]):
        text = ""
        if hasattr(comment, 'message') and comment.message:
            text = comment.message
        elif hasattr(comment, 'text') and comment.text:
            text = comment.text
        elif isinstance(comment, str):
            text = comment

        if text:
            clean = _clean_text(text)[:200]
            if clean and len(clean) > 5:
                texts.append(f"[{i+1}]: {clean}")

    return "\n".join(texts)


# === OLLAMA API ===

def _call_ollama(system_prompt: str, user_message: str) -> Optional[str]:
    """–ó–∞–ø—Ä–æ—Å –∫ Ollama"""
    payload = {
        "model": OLLAMA_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ],
        "stream": False,
        "think": False,
        "options": {
            "temperature": 0.3,
            "num_predict": 500
        }
    }

    try:
        response = requests.post(OLLAMA_URL, json=payload, timeout=OLLAMA_TIMEOUT)
        if response.status_code != 200:
            print(f"OLLAMA: HTTP {response.status_code}")
            return None

        data = response.json()
        content = data.get("message", {}).get("content", "").strip()
        return content

    except requests.exceptions.ConnectionError:
        print("OLLAMA: –ù–µ –∑–∞–ø—É—â–µ–Ω! –ó–∞–ø—É—Å—Ç–∏: ollama serve")
        return None
    except requests.exceptions.Timeout:
        print(f"OLLAMA: –¢–∞–π–º–∞—É—Ç ({OLLAMA_TIMEOUT} —Å–µ–∫)")
        return None
    except Exception as e:
        print(f"OLLAMA: –û—à–∏–±–∫–∞ - {e}")
        return None


# === COMMENT ANALYZER V2.0 ===

COMMENT_ANALYZER_SYSTEM = """You are a comment authenticity analyzer V2.0.
Detect bots vs real humans and measure audience trust.
V2.0: You also see POST CONTEXT - what posts are being commented on.
Be objective. Output ONLY valid JSON, no other text.
CRITICAL: Analyze ACTUAL comments. Give UNIQUE scores based on what you SEE."""

COMMENT_ANALYZER_PROMPT_V2 = """Analyze these Telegram channel comments WITH post context:

## POST CONTEXT (what is being commented):
{posts_context}

## COMMENTS:
{comments_text}

---
TASK V2.0: Analyze comments in context of the posts above.

1. AUTHENTICITY (0-100, where 100 = all real humans):
   Count each comment type:
   - BOT-like: generic praise ("Great!", "üëç", "üî•"), no specific content, repetitive
   - HUMAN-like: specific references to post content, personal stories, questions, debates

   Calculate: authenticity = 100 - (bot_comments / total * 100)

2. TRUST_SCORE (0-100):
   Look for trust signals in comments:
   - "Bought it" / "–ö—É–ø–∏–ª" / "–°–¥–µ–ª–∞–ª" = HIGH trust
   - Specific references to post content = HIGH trust
   - "Source?" / "Proof?" / "–û–ø—è—Ç—å —Ä–µ–∫–ª–∞–º–∞" = LOW trust
   - Generic "—Å–ø–∞—Å–∏–±–æ" without context = LOW trust

3. SARCASM DETECTION (V2.0):
   ‚ö†Ô∏è SUSPICIOUS if:
   - Post is negative/controversial BUT comments are all positive ‚Üí possible bot farm
   - Post makes bold claims BUT no one questions ‚Üí suspicious
   - Zero critical comments on divisive content ‚Üí suspicious

IMPORTANT: Do NOT use placeholder values. Count ACTUAL patterns.

Output JSON format:
{{"authenticity": <0-100>, "bot_percentage": <0-100>, "bot_signals": [<PATTERNS>], "trust_score": <0-100>, "trust_signals": [<SIGNALS>], "sarcasm_warning": <true/false>}}"""


def analyze_comments(comments: list, posts: list = None) -> Optional[CommentAnalysisResult]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∫–∞–Ω–∞–ª–∞ V2.0 —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø–æ—Å—Ç–æ–≤"""
    comments_text = _prepare_comments_text(comments)

    if not comments_text or len(comments_text) < 50:
        print("LLM CommentAnalyzer: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return None

    # V2.0: –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–æ–≤ –¥–ª—è sarcasm detection
    posts_context = "–ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ—Å—Ç–æ–≤"
    if posts:
        posts_context = _prepare_posts_text(posts[:10])[:2000]  # –ü–µ—Ä–≤—ã–µ 10 –ø–æ—Å—Ç–æ–≤

    prompt = COMMENT_ANALYZER_PROMPT_V2.format(
        posts_context=posts_context,
        comments_text=comments_text[:5000]
    )

    if DEBUG_LLM_ANALYZER:
        print(f"\n{'='*60}")
        print(f"COMMENT ANALYZER V2.0 - {len(comments)} comments, {len(comments_text)} chars")
        print(f"Posts context: {len(posts_context)} chars")
        print(f"{'='*60}\n")

    response = _call_ollama(COMMENT_ANALYZER_SYSTEM, prompt)

    if not response:
        return None

    if DEBUG_LLM_ANALYZER:
        print(f"COMMENT ANALYZER RESPONSE:\n{response[:500]}")

    # V2.0: Use safe_parse_json with fallback
    default_values = {
        "authenticity": 50,
        "bot_percentage": 50,
        "bot_signals": [],
        "trust_score": 50,
        "trust_signals": [],
        "sarcasm_warning": False
    }
    data, warnings = safe_parse_json(response, default_values)

    if DEBUG_LLM_ANALYZER and warnings:
        print(f"JSON PARSE WARNINGS:")
        for w in warnings:
            print(f"  - {w}")

    if data:
        return CommentAnalysisResult(
            authenticity=int(data.get("authenticity", 50)),
            bot_percentage=int(data.get("bot_percentage", 50)),
            bot_signals=data.get("bot_signals", []),
            trust_score=int(data.get("trust_score", 50)),
            trust_signals=data.get("trust_signals", []),
            raw_response=response
        )

    print(f"COMMENT ANALYZER: Failed to parse response")
    return None


# === –ì–õ–ê–í–ù–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† ===

class LLMAnalyzer:
    """–ü–æ–ª–Ω—ã–π LLM –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞"""

    def __init__(self):
        self.cache = _load_cache()
        print(f"LLM ANALYZER v37.0: Ollama ({OLLAMA_MODEL})")

    def analyze(
        self,
        channel_id: int,
        messages: list,
        comments: list,
        category: str = "DEFAULT"
    ) -> LLMAnalysisResult:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞–Ω–∞–ª–∞.

        Args:
            channel_id: ID –∫–∞–Ω–∞–ª–∞
            messages: –°–ø–∏—Å–æ–∫ –ø–æ—Å—Ç–æ–≤
            comments: –°–ø–∏—Å–æ–∫ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∫–∞–Ω–∞–ª–∞ (–¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–æ–∫)

        Returns:
            LLMAnalysisResult —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        result = LLMAnalysisResult(posts=None, comments=None)

        # V2.1: Post Analyzer –û–¢–ö–õ–Æ–ß–ï–ù ‚Äî –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        # –ß–µ–ª–æ–≤–µ–∫ —Å–∞–º –≤–∏–¥–∏—Ç —á—Ç–æ –∫–∞–Ω–∞–ª –ø—Ä–æ –ø–æ–ª–∏—Ç–∏–∫—É/–≤–æ–π–Ω—É
        # toxicity, violence, military_conflict ‚Äî –Ω–µ –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–µ–∫–ª–∞–º—É
        print(f"LLM: PostAnalyzer –æ—Ç–∫–ª—é—á–µ–Ω (V2.1 ‚Äî –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏)")

        # Comment Analyzer ‚Äî –ü–û–õ–ï–ó–ù–û: –¥–µ—Ç–µ–∫—Ü–∏—è –±–æ—Ç–æ–≤
        if comments and len(comments) >= 5:
            result.comments = analyze_comments(comments, posts=messages)
        else:
            print(f"LLM: –ü—Ä–æ–ø—É—Å–∫ CommentAnalyzer (–º–∞–ª–æ –∫–æ–º–º–µ–Ω—Ç–æ–≤: {len(comments) if comments else 0})")

        # V2.1: –£–ø—Ä–æ—â—ë–Ω–Ω—ã–π impact ‚Äî —Ç–æ–ª—å–∫–æ –æ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
        result.calculate_impact_v2()

        return result

    def save_cache(self):
        _save_cache(self.cache)


# === –¢–ï–°–¢–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø ===

def print_analysis_result(result: LLMAnalysisResult, channel_name: str = ""):
    """–ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ v37.0"""
    print(f"\n{'='*60}")
    print(f"LLM ANALYSIS v37.0: {channel_name}")
    print(f"{'='*60}")

    # v37.0: –¢–∏—Ä –∏ cap
    print(f"\nüìä SUITABILITY TIER: {result.tier} (cap={result.tier_cap})")
    if result.exclusion_reason:
        print(f"   ‚õî EXCLUDED: {result.exclusion_reason}")

    if result.posts:
        p = result.posts
        print(f"\nüìù POST ANALYSIS:")
        print(f"   Brand Safety: {p.brand_safety}/100")
        print(f"   - Toxicity: {p.toxicity}")
        print(f"   - Violence: {p.violence}")  # v37.0
        print(f"   - Political Quantity: {p.political_quantity}%")  # v37.0
        print(f"   - Political Risk: {p.political_risk}")  # v37.0
        print(f"   - Misinformation: {p.misinformation}")
        print(f"   Ad Percentage: {p.ad_percentage}%")
        if p.red_flags:
            print(f"   Red Flags: {p.red_flags}")
    else:
        print(f"\nüìù POST ANALYSIS: –ü—Ä–æ–ø—É—â–µ–Ω (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)")

    if result.comments:
        c = result.comments
        print(f"\nüí¨ COMMENT ANALYSIS:")
        print(f"   Authenticity: {c.authenticity}/100 ({100-c.bot_percentage}% –∂–∏–≤—ã–µ)")
        print(f"   Bot Signals: {c.bot_signals}")
        print(f"   Trust Score: {c.trust_score}/100")
        print(f"   Trust Signals: {c.trust_signals}")
    else:
        print(f"\nüí¨ COMMENT ANALYSIS: –ü—Ä–æ–ø—É—â–µ–Ω (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)")

    print(f"\nüìä IMPACT ON SCORE:")
    print(f"   LLM Bonus: +{result.llm_bonus:.1f} points")
    print(f"   LLM Trust Factor: √ó{result.llm_trust_factor:.2f}")
    print(f"   Tier Cap: {result.tier_cap}")

    # –ü—Ä–∏–º–µ—Ä –≤–ª–∏—è–Ω–∏—è —Å tier cap
    example_raw = 70
    example_trust = 0.95
    old_score = example_raw * example_trust
    base_new = (example_raw + result.llm_bonus) * example_trust * result.llm_trust_factor
    new_score = min(base_new, result.tier_cap)  # v37.0: –ø—Ä–∏–º–µ–Ω—è–µ–º cap
    print(f"\n   Example: Raw=70, Trust=0.95")
    print(f"   Old formula: {old_score:.1f}")
    print(f"   New formula: {base_new:.1f} ‚Üí capped to {new_score:.1f}")

    print(f"{'='*60}\n")
